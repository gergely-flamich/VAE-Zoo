{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0706 22:37:52.715366 139689796065024 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0706 22:37:52.731045 139689796065024 deprecation_wrapper.py:119] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/custom_getters/restore_initializer.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfs = tf.contrib.summary\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "import sonnet as snt\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_input_fn(data, batch_size=256, shuffle_samples=5000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(shuffle_samples)\n",
    "    dataset = dataset.map(mnist_parse_fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mnist_parse_fn(data):\n",
    "    return tf.cast(data, tf.float32) / 255.\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=True),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldVAE(snt.AbstractModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 latent_dim=32, \n",
    "                 name=\"manifold_vae\"):\n",
    "        \n",
    "        super(ManifoldVAE, self).__init__(name=name)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.is_training = True\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        \"\"\"\n",
    "        Calculates the KL divergence between the current variational posterior and the prior:\n",
    "\n",
    "        KL[ q(z | theta) || p(z) ]\n",
    "\n",
    "        \"\"\"\n",
    "        self._ensure_is_connected()\n",
    "        \n",
    "        return tfd.kl_divergence(self.latent_posterior, self.latent_prior)\n",
    "    \n",
    "    @property\n",
    "    def log_prob(self):\n",
    "        return tf.reduce_sum(self._log_prob)\n",
    "    \n",
    "    \n",
    "    @snt.reuse_variables\n",
    "    def encode(self, inputs, test_local_stats=True):\n",
    "        \"\"\"\n",
    "        Builds the encoder part of the VAE, i.e. q(x | theta).\n",
    "        This maps from the input to the latent representation.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        self.encoder_layers = [\n",
    "            snt.BatchReshape((28, 28, 1)),\n",
    "            snt.Conv2D(output_channels=64,\n",
    "                       kernel_shape=(5, 5),\n",
    "                       stride=2,\n",
    "                       name=\"encoder_conv1\"),\n",
    "            tf.nn.leaky_relu,\n",
    "            snt.Conv2D(output_channels=128,\n",
    "                       kernel_shape=(5, 5),\n",
    "                       stride=2,\n",
    "                       use_bias=False,\n",
    "                       name=\"encoder_conv2\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.nn.leaky_relu,\n",
    "            snt.BatchFlatten(),\n",
    "            snt.Linear(output_size=1024,\n",
    "                       use_bias=False,\n",
    "                       name=\"encoder_linear1\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.nn.leaky_relu\n",
    "        ]\n",
    "        \n",
    "        self.encoder_loc_head = snt.Linear(output_size=self.latent_dim,\n",
    "                                           name=\"encoder_loc_head\")\n",
    "        self.encoder_log_scale_head = snt.Linear(output_size=self.latent_dim,\n",
    "                                                 name=\"encoder_log_scale_head\")\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        activations = inputs\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                activations = layer(activations, \n",
    "                                    training=self.is_training)\n",
    "                \n",
    "            else:\n",
    "                activations = layer(activations)\n",
    "            \n",
    "        # Get latent posterior statistics\n",
    "        loc = self.encoder_loc_head(activations)\n",
    "        scale = 1e-6 + tf.nn.softplus(self.encoder_log_scale_head(activations))\n",
    "        \n",
    "        # Create latent posterior\n",
    "        self.latent_posterior = tfd.Normal(loc=loc, scale=scale)\n",
    "        \n",
    "        return self.latent_posterior.sample()\n",
    "\n",
    "\n",
    "    @snt.reuse_variables\n",
    "    def decode(self, latent_code, test_local_stats=True):\n",
    "        \"\"\"\n",
    "        Builds the decoder part of the VAE\n",
    "        \"\"\"\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        self.decoder_levels = [self.encoder_loc_head.transpose()]\n",
    "        \n",
    "        for layer in self.encoder_layers[::-1]:\n",
    "            \n",
    "            # Some layers need care for reversing\n",
    "            if isinstance(layer, snt.Transposable):\n",
    "                layer = layer.transpose()\n",
    "                \n",
    "            elif isinstance(layer, snt.BatchFlatten):\n",
    "                layer = BatchReshape((28, 28, 1))\n",
    "                \n",
    "            elif isinstance(layer, snt.BatchReshape):\n",
    "                continue\n",
    "           \n",
    "            # Add layer\n",
    "            self.decoder_levels.append(layer)\n",
    "        \n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        # Create prior\n",
    "        self.latent_prior = tfd.Normal(loc=tf.zeros_like(latent_code),\n",
    "                                       scale=tf.ones_like(latent_code))\n",
    "        \n",
    "        activations = latent_code\n",
    "        \n",
    "        for layer in self.decoder_levels:\n",
    "            if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                activations = layer(activations, \n",
    "                                    training=self.is_training)\n",
    "\n",
    "            else:\n",
    "                activations = layer(activations)\n",
    "        \n",
    "        return tf.nn.sigmoid(activations)\n",
    "    \n",
    "    \n",
    "    def _build(self, inputs, test_local_stats=True):\n",
    "        \"\"\"\n",
    "        Build standard VAE:\n",
    "        1. Encode input -> latent mu, sigma\n",
    "        2. Sample z ~ N(z | mu, sigma)\n",
    "        \"\"\"\n",
    "        \n",
    "        reshaper = snt.BatchReshape((28, 28, 1))\n",
    "        inputs = reshaper(inputs)\n",
    "        \n",
    "        # Code the latents on the first stage\n",
    "        latents = self.encode(inputs, test_local_stats=test_local_stats)\n",
    "        \n",
    "        # Reconsturct image from the latents\n",
    "        reconstruction = self.decode(latents, test_local_stats=test_local_stats)\n",
    "        \n",
    "        self.log_gamma = tf.get_variable(\"log_gamma_x\", [], tf.float32, tf.zeros_initializer())\n",
    "        self.likelihood = tfd.Normal(loc=reconstruction,\n",
    "                                     scale=tf.exp(self.log_gamma))\n",
    "        self._log_prob = self.likelihood.log_prob(inputs)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config, model_dir, is_training, train_first_stage=True):\n",
    "\n",
    "    num_batches = config[\"num_training_examples\"] // config[\"batch_size\"] + 1\n",
    "  \n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    ((train_data, _),\n",
    "     (eval_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create model\n",
    "    # ==========================================================================\n",
    "\n",
    "#     g = tf.get_default_graph()\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "    \n",
    "        vae = ManifoldVAE(latent_dim=64)\n",
    "        vae(tf.zeros((1, 28, 28)))\n",
    "        \n",
    "        optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "        # ==========================================================================\n",
    "        # Define Checkpoints\n",
    "        # ==========================================================================\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "        trainable_vars = vae.get_all_variables() + (global_step,)\n",
    "        checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "        checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "            variables=trainable_vars,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "        # ==========================================================================\n",
    "        # Define Tensorboard Summary writer\n",
    "        # ==========================================================================\n",
    "\n",
    "        logdir = os.path.join(model_dir, \"log\")\n",
    "        writer = tfs.create_file_writer(logdir)\n",
    "        writer.set_as_default()\n",
    "\n",
    "        # ==========================================================================\n",
    "        # Train the model\n",
    "        # ==========================================================================\n",
    "\n",
    "        if is_training:\n",
    "\n",
    "            if train_first_stage:\n",
    "                beta = config[\"beta1\"]\n",
    "\n",
    "                for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "                    dataset = mnist_input_fn(data=train_data,\n",
    "                                            batch_size=config[\"batch_size\"])\n",
    "\n",
    "                    with tqdm(total=num_batches) as pbar:\n",
    "                        for batch in dataset:\n",
    "                            # Increment global step\n",
    "                            global_step += 1\n",
    "\n",
    "                            # Record gradients of the forward pass\n",
    "                            with tf.GradientTape() as tape, tfs.record_summaries_every_n_global_steps(config[\"log_freq\"]):\n",
    "\n",
    "                                output = vae(batch)\n",
    "\n",
    "                                kl = vae.kl_divergence\n",
    "                                total_kl = tf.reduce_sum(kl)\n",
    "\n",
    "                                log_prob = vae.log_prob\n",
    "\n",
    "                                warmup_coef = tf.minimum(1., global_step.numpy() / (config[\"warmup\"] * num_batches))\n",
    "\n",
    "                                # negative ELBO\n",
    "                                loss = total_kl - beta * warmup_coef * log_prob \n",
    "\n",
    "                                output = tf.cast(output, tf.float32)\n",
    "\n",
    "                                # Add tensorboard summaries\n",
    "                                tfs.scalar(\"Loss\", loss)\n",
    "                                tfs.scalar(\"Total_KL\", kl)\n",
    "                                tfs.scalar(\"Max_KL\", tf.reduce_max(kl))\n",
    "                                tfs.scalar(\"Log-Probability\", log_prob)\n",
    "                                tfs.scalar(\"Warmup_Coef\", warmup_coef)\n",
    "                                tfs.scalar(\"Gamma-x\", tf.exp(vae.log_gamma))\n",
    "                                tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "                            print(update_ops)\n",
    "\n",
    "                            return\n",
    "\n",
    "                            # Backprop\n",
    "                            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                                grads = tape.gradient(loss, vae.get_all_variables())\n",
    "                                optimizer.apply_gradients(zip(grads, vae.get_all_variables()))\n",
    "\n",
    "                            # Update the progress bar\n",
    "                            pbar.update(1)\n",
    "                            pbar.set_description(\"Epoch {}, ELBO: {:.2f}\".format(epoch, loss))\n",
    "\n",
    "                    checkpoint.save(ckpt_prefix)\n",
    "\n",
    "                tfs.flush(writer)\n",
    "                print(\"First Stage Training Complete!\")\n",
    "            \n",
    "    tfs.graph(g)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/tmp/2-stage-vae-v2/\"\n",
    "\n",
    "config = {\n",
    "        \"num_training_examples\": 60000,\n",
    "        \"batch_size\": 250,\n",
    "        \"num_epochs\": 10,\n",
    "        \"num_epochs_stage_2\": 5,\n",
    "        \n",
    "        \"beta1\": 0.1,\n",
    "        \"beta2\": 0.1,\n",
    "        \"warmup\": 10.,\n",
    "        \n",
    "        \"learning_rate\": 1e-3,\n",
    "        \n",
    "        \"optimizer\": \"adam\",\n",
    "        \n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "        \"log_freq\": 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 250,\n",
      "    \"beta1\": 0.1,\n",
      "    \"beta2\": 0.1,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"log_freq\": 100,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_epochs_stage_2\": 5,\n",
      "    \"num_training_examples\": 60000,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"warmup\": 10.0\n",
      "}\n",
      "No checkpoint found!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96006adca11d49f79451b65f98578a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0706 22:38:40.863915 139689796065024 variables.py:2429] Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainable variable created when calling a template after the first time, perhaps you used tf.Variable when you meant tf.get_variable: [<tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_2/gamma:0' shape=(128,) dtype=float32>, <tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_2/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_3/gamma:0' shape=(1024,) dtype=float32>, <tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_3/beta:0' shape=(1024,) dtype=float32>]\n\noriginally defined at:\n  File \"<ipython-input-7-dcd14f9abfbe>\", line 23, in run\n    vae = ManifoldVAE(latent_dim=64)\n  File \"<ipython-input-3-76f556e2b9ad>\", line 7, in __init__\n    super(ManifoldVAE, self).__init__(name=name)\n  File \"/homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\", line 180, in __init__\n    custom_getter_=self._custom_getter)\n  File \"/homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\", line 160, in make_template\n    **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a3756a3969f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_first_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-dcd14f9abfbe>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config, model_dir, is_training, train_first_stage)\u001b[0m\n\u001b[1;32m     70\u001b[0m                             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_summaries_every_n_global_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_freq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                                 \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraph_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m       with variable_scope.variable_scope(\n\u001b[1;32m    383\u001b[0m           self._variable_scope, reuse=not self._first_call):\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m       \u001b[0;31m# The scope was not created at construction time, so create it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m_call_func\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                            \u001b[0;34m\"after the first time, perhaps you used tf.Variable \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                            \u001b[0;34m\"when you meant tf.get_variable: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                            (trainable_variables[trainable_at_start:],))\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;31m# Non-trainable tracking variables are a legitimate reason why a new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trainable variable created when calling a template after the first time, perhaps you used tf.Variable when you meant tf.get_variable: [<tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_2/gamma:0' shape=(128,) dtype=float32>, <tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_2/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_3/gamma:0' shape=(1024,) dtype=float32>, <tf.Variable 'manifold_vae_1/manifold_vae/encode/batch_normalization_3/beta:0' shape=(1024,) dtype=float32>]\n\noriginally defined at:\n  File \"<ipython-input-7-dcd14f9abfbe>\", line 23, in run\n    vae = ManifoldVAE(latent_dim=64)\n  File \"<ipython-input-3-76f556e2b9ad>\", line 7, in __init__\n    super(ManifoldVAE, self).__init__(name=name)\n  File \"/homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\", line 180, in __init__\n    custom_getter_=self._custom_getter)\n  File \"/homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\", line 160, in make_template\n    **kwargs)\n"
     ]
    }
   ],
   "source": [
    "vae = run(config, model_dir=MODEL_DIR, is_training=True, train_first_stage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_data, _),\n",
    "(eval_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "vae.is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf8609b390>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = vae.latent_posterior.sample()\n",
    "test_im = mnist_parse_fn(train_data[9])\n",
    "z_ = vae.encode(test_im[tf.newaxis, ...], test_local_stats=False)\n",
    "\n",
    "im = vae.decode(z, test_local_stats=False)\n",
    "print(im.shape)\n",
    "im = tf.squeeze(im).numpy()\n",
    "\n",
    "print(vae.is_training)\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(im)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.5966993   0.24020544 -1.0628476  -0.27744395 -0.62909436  0.3334056\n",
      "  -1.4285717  -0.4879048   1.1275315  -0.4965166   1.4587637  -0.42441043\n",
      "   0.3963925   1.0481342   0.23841102 -0.37203413  1.2964473  -0.9027144\n",
      "   1.2561873   0.7882661  -1.1573831  -1.7816384  -1.3260957   0.7349494\n",
      "   0.9542781  -0.7522101  -0.05666764  0.0938426   0.16593735 -0.12231594\n",
      "   0.76784265 -0.00784632  0.8623152  -0.93366295 -0.12255742  0.6166265\n",
      "   0.4171484  -0.00670139 -0.0830364   0.14931692 -1.0290347   0.35885155\n",
      "  -0.44689992  1.1856756   0.9066683  -0.7671778  -0.48246858 -0.547666\n",
      "  -1.4224062  -1.355649   -0.49032408 -0.21278995 -0.5402432  -0.0758155\n",
      "   0.3474331  -1.2097613  -0.40928254 -1.384895    1.6998756   0.20649637\n",
      "  -0.12069284  0.2758879   0.4624661   0.51384795]], shape=(1, 64), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 3.9798164e+00  2.7631283e+00 -5.5554042e+00  2.0732424e+01\n",
      "  -1.1920581e+01  1.9174673e+01 -4.6818151e+00  6.5271020e+00\n",
      "   1.6129889e+01 -3.9901242e-01 -3.2115602e+00  2.6565740e+00\n",
      "  -4.0340843e+00  1.1812730e+00  2.0698984e+00 -2.1491933e+00\n",
      "  -1.2701288e+01  3.8048449e-01 -7.8397284e+00 -2.0269100e+01\n",
      "   9.9539441e-01  1.2112578e+01 -4.6723590e+00  5.1957996e+01\n",
      "   1.1653450e+01  4.2307892e+00  9.1607428e+00 -8.6041374e+00\n",
      "  -1.9970827e+01 -6.3065338e+00  2.1365342e+00 -4.6188092e-01\n",
      "   4.4964395e+00  2.5321552e-01  8.4775686e-03 -1.3490792e+01\n",
      "  -1.4995946e+00 -1.4038302e+01  6.1249802e+01  2.0131121e+00\n",
      "   6.9712443e+00 -6.9284782e+00  1.2460683e+01  1.3383782e+01\n",
      "  -5.3809977e+00 -3.2730939e+00  1.5095879e+01  7.4030886e+00\n",
      "  -1.1576117e+01  1.0661006e+01  9.8149137e+00  8.0647535e+00\n",
      "  -7.4155288e+00 -1.0345033e+01  2.5489211e+00 -3.4995049e+01\n",
      "   2.3875347e+01 -1.2761585e+00 -7.7732482e+00 -1.4325291e+01\n",
      "   3.8231487e+00  1.2067083e+01  1.1683576e+01  3.0968828e+00]], shape=(1, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "print(z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1927559, shape=(28, 28), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.7411765 , 0.74509805, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.56078434, 0.96862745, 0.6       , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.53333336, 0.96862745, 0.9490196 , 0.3372549 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.7529412 , 0.9882353 , 0.73333335, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24313726, 0.7254902 , 0.07058824,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34901962,\n",
       "        0.9254902 , 0.8509804 , 0.18431373, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.84705883, 0.99215686, 0.23529412,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.83137256,\n",
       "        1.        , 0.31764707, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.80784315, 0.9882353 , 0.26666668,\n",
       "        0.        , 0.        , 0.        , 0.1882353 , 0.9490196 ,\n",
       "        0.99215686, 0.34901962, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5137255 , 0.9843137 , 0.83137256, 0.08235294,\n",
       "        0.        , 0.        , 0.04313726, 0.654902  , 0.9882353 ,\n",
       "        0.77254903, 0.01960784, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11372549, 0.9098039 , 0.96862745, 0.24705882, 0.        ,\n",
       "        0.        , 0.        , 0.6       , 0.9882353 , 0.8862745 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.85882354, 0.9882353 , 0.56078434, 0.        , 0.        ,\n",
       "        0.        , 0.45490196, 0.9764706 , 0.9882353 , 0.40392157,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01568628, 0.3764706 , 0.99215686,\n",
       "        1.        , 0.99215686, 0.78431374, 0.47843137, 0.02745098,\n",
       "        0.09803922, 0.7882353 , 0.98039216, 0.61960787, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.36078432, 0.9882353 , 0.9882353 ,\n",
       "        0.99215686, 0.8509804 , 0.9882353 , 0.9882353 , 0.78431374,\n",
       "        0.8901961 , 0.9882353 , 0.90588236, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34117648, 0.9843137 , 0.96862745, 0.90588236,\n",
       "        0.25490198, 0.1882353 , 0.7411765 , 0.9882353 , 0.9882353 ,\n",
       "        0.99215686, 0.9882353 , 0.9843137 , 0.8901961 , 0.13725491,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.74509805, 0.8666667 , 0.38431373, 0.        ,\n",
       "        0.        , 0.        , 0.16470589, 0.76862746, 0.9882353 ,\n",
       "        0.99215686, 0.9882353 , 0.9882353 , 0.63529414, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.43529412, 0.11372549, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24313726, 0.9372549 , 0.9882353 ,\n",
       "        0.3372549 , 0.16470589, 0.16470589, 0.05490196, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05882353, 0.5803922 , 0.99215686, 0.85490197,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.4745098 , 0.9882353 , 0.90588236, 0.10980392,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.12156863, 0.8666667 , 0.9843137 , 0.5058824 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.85490197, 0.9882353 , 0.627451  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.47843137, 0.9882353 , 0.32156864, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.5376430e-02, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.9918354e-01, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 4.7276825e-02],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 9.9990481e-01, 1.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.9999529e-01,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.3411045e-06,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 9.9997067e-01, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 9.9914157e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        9.9999321e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00],\n",
       "       [9.9990994e-01, 1.2660027e-04, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 7.7123725e-01, 1.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 9.6235514e-01, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.9999678e-01],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.9999917e-01],\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 3.9551854e-02, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 6.0015035e-01, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.9999988e-01, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        1.4702380e-03, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.4235318e-02, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 9.9483389e-01, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 9.9888974e-01, 1.0000000e+00, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1929653, shape=(1, 64), dtype=float32, numpy=\n",
       "array([[-0.5581691 ,  0.01775781, -0.6093106 , -1.0657898 , -0.63679683,\n",
       "        -0.16028693,  0.02292752,  0.7399033 , -0.5489996 , -0.49665543,\n",
       "        -1.0602268 , -1.6546901 ,  0.34913424, -0.19600882,  1.0969449 ,\n",
       "        -0.4485845 ,  1.6164017 , -0.48646978,  0.5601295 ,  0.2864341 ,\n",
       "        -0.78044325,  1.0218213 , -0.2592898 , -0.8651161 ,  0.24365303,\n",
       "        -0.06692692,  0.11266441,  0.280285  , -0.8611255 , -0.83919936,\n",
       "        -0.8793286 ,  0.26535162,  0.5393854 ,  0.4650734 , -1.1195016 ,\n",
       "        -0.3236745 ,  0.15036449,  0.5745703 , -0.00505354,  0.09977586,\n",
       "        -0.2344154 , -0.07438146, -1.6192576 , -0.76564246, -0.23782364,\n",
       "         0.07905169, -0.86064935, -0.7772483 ,  0.03899552, -1.4230525 ,\n",
       "         0.38040376,  0.6333171 ,  0.0978348 ,  0.8511952 ,  1.3628192 ,\n",
       "        -0.39277148, -0.10434239, -0.5582481 , -0.6518381 ,  0.26644206,\n",
       "        -0.81384236, -0.05203085,  0.06435593, -0.2363559 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
