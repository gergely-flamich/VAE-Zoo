{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "tfs = tf.contrib.summary\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore\n",
    "from hierarchical_vae import HierarchicalVAE\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def mnist_input_fn(data, batch_size=256, shuffle_samples=5000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(shuffle_samples)\n",
    "    dataset = dataset.map(mnist_parse_fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mnist_parse_fn(data):\n",
    "    return tf.cast(data, tf.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=True),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model_dir,\n",
    "        is_training):\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Configuration\n",
    "    # ==========================================================================\n",
    "\n",
    "    config = {\n",
    "        \"num_training_examples\": 60000,\n",
    "        \"batch_size\": 250,\n",
    "        \"num_epochs\": 40,\n",
    "        \n",
    "        \"beta\": 1.,\n",
    "        \"warmup\": 20.,\n",
    "        \n",
    "        \"learning_rate\": 1e-3,\n",
    "        \n",
    "        \"optimizer\": \"adam\",\n",
    "        \n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "        \"log_freq\": 100,\n",
    "    }\n",
    "\n",
    "    num_batches = config[\"num_training_examples\"] // config[\"batch_size\"] + 1\n",
    "    \n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    ((train_data, _),\n",
    "     (eval_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create model\n",
    "    # ==========================================================================\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    \n",
    "    with g.as_default():\n",
    "    \n",
    "        hier_vae = HierarchicalVAE(hidden_sizes=(512, 256, 128),\n",
    "                                   latent_sizes=(64, 32, 16),\n",
    "                                   output_size=(28, 28),\n",
    "                                   latent_dist=\"laplace\",\n",
    "                                   likelihood=\"laplace\",\n",
    "                                   standardized=True)\n",
    "        hier_vae(tf.zeros((1, 28, 28)))\n",
    "        \n",
    "        del hier_vae\n",
    "        \n",
    "    hier_vae = HierarchicalVAE(hidden_sizes=(512, 256, 128),\n",
    "                                   latent_sizes=(64, 32, 16),\n",
    "                                   output_size=(28, 28),\n",
    "                                   latent_dist=\"laplace\",\n",
    "                                   likelihood=\"laplace\",\n",
    "                                   standardized=True)\n",
    "    hier_vae(tf.zeros((1, 28, 28)))\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Checkpoints\n",
    "    # ==========================================================================\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    trainable_vars = hier_vae.get_all_variables() + (global_step,)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "        variables=trainable_vars,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Tensorboard Summary writer\n",
    "    # ==========================================================================\n",
    "\n",
    "    logdir = os.path.join(model_dir, \"log\")\n",
    "    writer = tfs.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    tfs.graph(g)\n",
    "    tfs.flush(writer)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Train the model\n",
    "    # ==========================================================================\n",
    "\n",
    "    beta = config[\"beta\"]\n",
    "\n",
    "    if is_training:\n",
    "        for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "            dataset = mnist_input_fn(data=train_data,\n",
    "                                    batch_size=config[\"batch_size\"])\n",
    "\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for batch in dataset:\n",
    "                    # Increment global step\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                    # Record gradients of the forward pass\n",
    "                    with tf.GradientTape() as tape, tfs.record_summaries_every_n_global_steps(config[\"log_freq\"]):\n",
    "\n",
    "                        output = hier_vae(batch)\n",
    "\n",
    "                        kl = hier_vae.kl_divergence \n",
    "                        log_prob = hier_vae.input_log_prob\n",
    "\n",
    "                        warmup_coef = tf.minimum(1., global_step.numpy() / (config[\"warmup\"] * num_batches))\n",
    "\n",
    "                        # negative ELBO\n",
    "                        loss = kl - beta * warmup_coef * log_prob \n",
    "\n",
    "                        output = tf.cast(tf.expand_dims(output, axis=-1), tf.float32)\n",
    "\n",
    "                        # Add tensorboard summaries\n",
    "                        tfs.scalar(\"Loss\", loss)\n",
    "                        tfs.scalar(\"KL\", kl)\n",
    "                        tfs.scalar(\"Log-Probability\", log_prob)\n",
    "                        tfs.scalar(\"Warmup_Coef\", warmup_coef)\n",
    "                        tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                    # Backprop\n",
    "                    grads = tape.gradient(loss, hier_vae.get_all_variables())\n",
    "                    optimizer.apply_gradients(zip(grads, hier_vae.get_all_variables()))\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(\"Epoch {}, ELBO: {:.2f}\".format(epoch, loss))\n",
    "\n",
    "            checkpoint.save(ckpt_prefix)\n",
    "\n",
    "    return hier_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 250,\n",
      "    \"beta\": 1.0,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"log_freq\": 100,\n",
      "    \"num_epochs\": 40,\n",
      "    \"num_training_examples\": 60000,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"warmup\": 20.0\n",
      "}\n",
      "No checkpoint found!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dfb121eeb24457ae574e7b8bca3a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bbff075aadc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m hier_vae = run(model_dir=\"/tmp/hierarchical_vae_laplace_with_warmup\",\n\u001b[0;32m----> 2\u001b[0;31m                is_training=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9fa8b33980f6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model_dir, is_training)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_summaries_every_n_global_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_freq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhier_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhier_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraph_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_scope_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0;31m# The scope was not created at construction time, so create it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m_call_func\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m       \u001b[0mtrainable_at_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables_created\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;31m# The first time we run, restore variables if necessary (via\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;31m# Make a dummy subscope to check the name scope we are in. We could read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# the name scope from one of the outputs produced, except that the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/code/hierarchical_vae.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         latents = self.encode(inputs,\n\u001b[0;32m--> 206\u001b[0;31m                               level=self._num_levels)\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         decoded_loc, decoded_scale = self.decode(latents,\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/util.py\u001b[0m in \u001b[0;36meager_test\u001b[0;34m(method, obj, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m           \"modules.\")\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mwrapt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/util.py\u001b[0m in \u001b[0;36mcall_method\u001b[0;34m(method, obj, args, kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_capture_variables\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m               \u001b[0mout_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0mout_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/code/hierarchical_vae.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, inputs, level)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latent_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraph_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_scope_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0;31m# The scope was not created at construction time, so create it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m_call_func\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# Checkpointable).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcheckpointable_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables_created\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;31m# Make a dummy subscope to check the name scope we are in. We could read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# the name scope from one of the outputs produced, except that the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/code/hierarchical_vae.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_batch_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loc_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraph_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_scope_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_template_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0;31m# The scope was not created at construction time, so create it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/template.py\u001b[0m in \u001b[0;36m_call_func\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# Checkpointable).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcheckpointable_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables_created\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;31m# Make a dummy subscope to check the name scope we are in. We could read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# the name scope from one of the outputs produced, except that the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/batch_norm.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, input_batch, is_training, test_local_stats)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;31m# Sets up the batch normalization op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     out, mean, variance = self._batch_norm_op(input_batch, mean, variance,\n\u001b[0;32m--> 543\u001b[0;31m                                               use_batch_stats, stat_dtype)\n\u001b[0m\u001b[1;32m    544\u001b[0m     \u001b[0;31m# Sets up the update op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_update_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/batch_norm.py\u001b[0m in \u001b[0;36m_batch_norm_op\u001b[0;34m(self, input_batch, mean, variance, use_batch_stats, stat_dtype)\u001b[0m\n\u001b[1;32m    433\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m           name=\"batch_norm\")\n\u001b[0m\u001b[1;32m    436\u001b[0m       \u001b[0;31m# We'll echo the supplied mean and variance so that they can also be used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0;31m# to update the moving statistics. Cast to matching type if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[0;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[0;31m# Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[0;31m# the precise order of ops that are generated by the expression below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n\u001b[0m\u001b[1;32m   1112\u001b[0m         offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1076\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   5843\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5844\u001b[0;31m         _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[1;32m   5845\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hier_vae = run(model_dir=\"/tmp/hierarchical_vae_laplace_with_warmup\",\n",
    "               is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 250,\n",
      "    \"beta\": 1.0,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"log_freq\": 100,\n",
      "    \"num_epochs\": 40,\n",
      "    \"num_training_examples\": 60000,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"warmup\": 20.0\n",
      "}\n",
      "WARNING:tensorflow:From /Users/gergelyflamich/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Checkpoint found at /tmp/hierarchical_vae_laplace_with_warmup/checkpoints/_ckpt-40, restoring...\n",
      "Model restored!\n"
     ]
    }
   ],
   "source": [
    "hier_vae = run(model_dir=\"/tmp/hierarchical_vae_laplace_with_warmup\",\n",
    "               is_training=False)\n",
    "\n",
    "hier_vae(tf.zeros((1, 28, 28)))\n",
    "\n",
    "hier_vae.training_finished()\n",
    "\n",
    "((train_data, _),\n",
    "(eval_data, _)) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124db30f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hier_vae.training_finished()\n",
    "\n",
    "res = hier_vae(tf.convert_to_tensor(train_data[:1] / 255., dtype=tf.float32))\n",
    "\n",
    "res_img = tf.squeeze(res).numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(res_img)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hier_vae._encoding_layers[0]._is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Graph in module tensorflow.python.framework.ops object:\n",
      "\n",
      "class Graph(builtins.object)\n",
      " |  A TensorFlow computation, represented as a dataflow graph.\n",
      " |  \n",
      " |  A `Graph` contains a set of\n",
      " |  `tf.Operation` objects,\n",
      " |  which represent units of computation; and\n",
      " |  `tf.Tensor` objects, which represent\n",
      " |  the units of data that flow between operations.\n",
      " |  \n",
      " |  A default `Graph` is always registered, and accessible by calling\n",
      " |  `tf.get_default_graph`.\n",
      " |  To add an operation to the default graph, simply call one of the functions\n",
      " |  that defines a new `Operation`:\n",
      " |  \n",
      " |  ```python\n",
      " |  c = tf.constant(4.0)\n",
      " |  assert c.graph is tf.get_default_graph()\n",
      " |  ```\n",
      " |  \n",
      " |  Another typical usage involves the\n",
      " |  `tf.Graph.as_default`\n",
      " |  context manager, which overrides the current default graph for the\n",
      " |  lifetime of the context:\n",
      " |  \n",
      " |  ```python\n",
      " |  g = tf.Graph()\n",
      " |  with g.as_default():\n",
      " |    # Define operations and tensors in `g`.\n",
      " |    c = tf.constant(30.0)\n",
      " |    assert c.graph is g\n",
      " |  ```\n",
      " |  \n",
      " |  Important note: This class *is not* thread-safe for graph construction. All\n",
      " |  operations should be created from a single thread, or external\n",
      " |  synchronization must be provided. Unless otherwise specified, all methods\n",
      " |  are not thread-safe.\n",
      " |  \n",
      " |  A `Graph` instance supports an arbitrary number of \"collections\"\n",
      " |  that are identified by name. For convenience when building a large\n",
      " |  graph, collections can store groups of related objects: for\n",
      " |  example, the `tf.Variable` uses a collection (named\n",
      " |  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n",
      " |  all variables that are created during the construction of a graph. The caller\n",
      " |  may define additional collections by specifying a new name.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Creates a new, empty Graph.\n",
      " |  \n",
      " |  add_to_collection(self, name, value)\n",
      " |      Stores `value` in the collection with the given `name`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collection.\n",
      " |  \n",
      " |  add_to_collections(self, names, value)\n",
      " |      Stores `value` in the collections given by `names`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times. This function makes sure that duplicates in\n",
      " |      `names` are ignored, but it will not check for pre-existing membership of\n",
      " |      `value` in any of the collections in `names`.\n",
      " |      \n",
      " |      `names` can be any iterable, but if `names` is a string, it is treated as a\n",
      " |      single collection name.\n",
      " |      \n",
      " |      Args:\n",
      " |        names: The keys for the collections to add to. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collections.\n",
      " |  \n",
      " |  as_default(self)\n",
      " |      Returns a context manager that makes this `Graph` the default graph.\n",
      " |      \n",
      " |      This method should be used if you want to create multiple graphs\n",
      " |      in the same process. For convenience, a global default graph is\n",
      " |      provided, and all ops will be added to this graph if you do not\n",
      " |      create a new graph explicitly.\n",
      " |      \n",
      " |      Use this method with the `with` keyword to specify that ops created within\n",
      " |      the scope of a block should be added to this graph. In this case, once\n",
      " |      the scope of the `with` is exited, the previous default graph is set again\n",
      " |      as default. There is a stack, so it's ok to have multiple nested levels\n",
      " |      of `as_default` calls.\n",
      " |      \n",
      " |      The default graph is a property of the current thread. If you\n",
      " |      create a new thread, and wish to use the default graph in that\n",
      " |      thread, you must explicitly add a `with g.as_default():` in that\n",
      " |      thread's function.\n",
      " |      \n",
      " |      The following code examples are equivalent:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 1. Using Graph.as_default():\n",
      " |      g = tf.Graph()\n",
      " |      with g.as_default():\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      \n",
      " |      # 2. Constructing and making default:\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      ```\n",
      " |      \n",
      " |      If eager execution is enabled ops created under this context manager will be\n",
      " |      added to the graph instead of executed eagerly.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for using this graph as the default graph.\n",
      " |  \n",
      " |  as_graph_def(self, from_version=None, add_shapes=False)\n",
      " |      Returns a serialized `GraphDef` representation of this graph.\n",
      " |      \n",
      " |      The serialized `GraphDef` can be imported into another `Graph`\n",
      " |      (using `tf.import_graph_def`) or used with the\n",
      " |      [C++ Session API](../../api_docs/cc/index.md).\n",
      " |      \n",
      " |      This method is thread-safe.\n",
      " |      \n",
      " |      Args:\n",
      " |        from_version: Optional.  If this is set, returns a `GraphDef`\n",
      " |          containing only the nodes that were added to this graph since\n",
      " |          its `version` property had the given value.\n",
      " |        add_shapes: If true, adds an \"_output_shapes\" list attr to each\n",
      " |          node with the inferred shapes of each of its outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A\n",
      " |        [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)\n",
      " |        protocol buffer.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the `graph_def` would be too large.\n",
      " |  \n",
      " |  as_graph_element(self, obj, allow_tensor=True, allow_operation=True)\n",
      " |      Returns the object referred to by `obj`, as an `Operation` or `Tensor`.\n",
      " |      \n",
      " |      This function validates that `obj` represents an element of this\n",
      " |      graph, and gives an informative error message if it is not.\n",
      " |      \n",
      " |      This function is the canonical way to get/validate an object of\n",
      " |      one of the allowed types from an external argument reference in the\n",
      " |      Session API.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        obj: A `Tensor`, an `Operation`, or the name of a tensor or operation.\n",
      " |          Can also be any object with an `_as_graph_element()` method that returns\n",
      " |          a value of one of these types.\n",
      " |        allow_tensor: If true, `obj` may refer to a `Tensor`.\n",
      " |        allow_operation: If true, `obj` may refer to an `Operation`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` or `Operation` in the Graph corresponding to `obj`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `obj` is not a type we support attempting to convert\n",
      " |          to types.\n",
      " |        ValueError: If `obj` is of an appropriate type but invalid. For\n",
      " |          example, an invalid string.\n",
      " |        KeyError: If `obj` is not an object in the graph.\n",
      " |  \n",
      " |  clear_collection(self, name)\n",
      " |      Clears all values in a collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class contains many\n",
      " |          standard names for collections.\n",
      " |  \n",
      " |  colocate_with(self, op, ignore_existing=False)\n",
      " |      Returns a context manager that specifies an op to colocate with.\n",
      " |      \n",
      " |      Note: this function is not for public use, only for internal libraries.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = tf.Variable([1.0])\n",
      " |      with g.colocate_with(a):\n",
      " |        b = tf.constant(1.0)\n",
      " |        c = tf.add(a, b)\n",
      " |      ```\n",
      " |      \n",
      " |      `b` and `c` will always be colocated with `a`, no matter where `a`\n",
      " |      is eventually placed.\n",
      " |      \n",
      " |      **NOTE** Using a colocation scope resets any existing device constraints.\n",
      " |      \n",
      " |      If `op` is `None` then `ignore_existing` must be `True` and the new\n",
      " |      scope resets all colocation and device constraints.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: The op to colocate all created ops with, or `None`.\n",
      " |        ignore_existing: If true, only applies colocation of this op within\n",
      " |          the context, rather than applying all colocation properties\n",
      " |          on the stack.  If `op` is `None`, this value must be `True`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if op is None but ignore_existing is False.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the op with which to colocate\n",
      " |        newly created ops.\n",
      " |  \n",
      " |  container(self, container_name)\n",
      " |      Returns a context manager that specifies the resource container to use.\n",
      " |      \n",
      " |      Stateful operations, such as variables and queues, can maintain their\n",
      " |      states on devices so that they can be shared by multiple processes.\n",
      " |      A resource container is a string name under which these stateful\n",
      " |      operations are tracked. These resources can be released or cleared\n",
      " |      with `tf.Session.reset()`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.container('experiment0'):\n",
      " |        # All stateful Operations constructed in this context will be placed\n",
      " |        # in resource container \"experiment0\".\n",
      " |        v1 = tf.Variable([1.0])\n",
      " |        v2 = tf.Variable([2.0])\n",
      " |        with g.container(\"experiment1\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # placed in resource container \"experiment1\".\n",
      " |          v3 = tf.Variable([3.0])\n",
      " |          q1 = tf.FIFOQueue(10, tf.float32)\n",
      " |        # All stateful Operations constructed in this context will be\n",
      " |        # be created in the \"experiment0\".\n",
      " |        v4 = tf.Variable([4.0])\n",
      " |        q1 = tf.FIFOQueue(20, tf.float32)\n",
      " |        with g.container(\"\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # be placed in the default resource container.\n",
      " |          v5 = tf.Variable([5.0])\n",
      " |          q3 = tf.FIFOQueue(30, tf.float32)\n",
      " |      \n",
      " |      # Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n",
      " |      # will become undefined (such as uninitialized).\n",
      " |      tf.Session.reset(target, [\"experiment0\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        container_name: container name string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for defining resource containers for stateful ops,\n",
      " |          yields the container name.\n",
      " |  \n",
      " |  control_dependencies(self, control_inputs)\n",
      " |      Returns a context manager that specifies control dependencies.\n",
      " |      \n",
      " |      Use with the `with` keyword to specify that all operations constructed\n",
      " |      within the context should have control dependencies on\n",
      " |      `control_inputs`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b, c]):\n",
      " |        # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n",
      " |        d = ...\n",
      " |        e = ...\n",
      " |      ```\n",
      " |      \n",
      " |      Multiple calls to `control_dependencies()` can be nested, and in\n",
      " |      that case a new `Operation` will have control dependencies on the union\n",
      " |      of `control_inputs` from all active contexts.\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies([c, d]):\n",
      " |          # Ops constructed here run after `a`, `b`, `c`, and `d`.\n",
      " |      ```\n",
      " |      \n",
      " |      You can pass None to clear the control dependencies:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies(None):\n",
      " |          # Ops constructed here run normally, not waiting for either `a` or `b`.\n",
      " |          with g.control_dependencies([c, d]):\n",
      " |            # Ops constructed here run after `c` and `d`, also not waiting\n",
      " |            # for either `a` or `b`.\n",
      " |      ```\n",
      " |      \n",
      " |      *N.B.* The control dependencies context applies *only* to ops that\n",
      " |      are constructed within the context. Merely using an op or tensor\n",
      " |      in the context does not add a control dependency. The following\n",
      " |      example illustrates this point:\n",
      " |      \n",
      " |      ```python\n",
      " |      # WRONG\n",
      " |      def my_func(pred, tensor):\n",
      " |        t = tf.matmul(tensor, tensor)\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created outside the context, so no control\n",
      " |          # dependency will be added.\n",
      " |          return t\n",
      " |      \n",
      " |      # RIGHT\n",
      " |      def my_func(pred, tensor):\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created in the context, so a control dependency\n",
      " |          # will be added.\n",
      " |          return tf.matmul(tensor, tensor)\n",
      " |      ```\n",
      " |      \n",
      " |      Also note that though execution of ops created under this scope will trigger\n",
      " |      execution of the dependencies, the ops created under this scope might still\n",
      " |      be pruned from a normal tensorflow graph. For example, in the following\n",
      " |      snippet of code the dependencies are never executed:\n",
      " |      \n",
      " |      ```python\n",
      " |        loss = model.loss()\n",
      " |        with tf.control_dependencies(dependencies):\n",
      " |          loss = loss + tf.constant(1)  # note: dependencies ignored in the\n",
      " |                                        # backward pass\n",
      " |        return tf.gradients(loss, model.variables)\n",
      " |      ```\n",
      " |      \n",
      " |      This is because evaluating the gradient graph does not require evaluating\n",
      " |      the constant(1) op created in the forward pass.\n",
      " |      \n",
      " |      Args:\n",
      " |        control_inputs: A list of `Operation` or `Tensor` objects which\n",
      " |          must be executed or computed before running the operations\n",
      " |          defined in the context.  Can also be `None` to clear the control\n",
      " |          dependencies.\n",
      " |      \n",
      " |      Returns:\n",
      " |       A context manager that specifies control dependencies for all\n",
      " |       operations constructed within the context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `control_inputs` is not a list of `Operation` or\n",
      " |          `Tensor` objects.\n",
      " |  \n",
      " |  create_op(self, op_type, inputs, dtypes, input_types=None, name=None, attrs=None, op_def=None, compute_shapes=True, compute_device=True)\n",
      " |      Creates an `Operation` in this graph. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(compute_shapes)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      " |      \n",
      " |      This is a low-level interface for creating an `Operation`. Most\n",
      " |      programs will not call this method directly, and instead use the\n",
      " |      Python op constructors, such as `tf.constant()`, which add ops to\n",
      " |      the default graph.\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type: The `Operation` type to create. This corresponds to the\n",
      " |          `OpDef.name` field for the proto that defines the operation.\n",
      " |        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n",
      " |        dtypes: A list of `DType` objects that will be the types of the tensors\n",
      " |          that the operation produces.\n",
      " |        input_types: (Optional.) A list of `DType`s that will be the types of\n",
      " |          the tensors that the operation consumes. By default, uses the base\n",
      " |          `DType` of each input in `inputs`. Operations that expect\n",
      " |          reference-typed inputs must specify `input_types` explicitly.\n",
      " |        name: (Optional.) A string name for the operation. If not specified, a\n",
      " |          name is generated based on `op_type`.\n",
      " |        attrs: (Optional.) A dictionary where the key is the attribute name (a\n",
      " |          string) and the value is the respective `attr` attribute of the\n",
      " |          `NodeDef` proto that will represent the operation (an `AttrValue`\n",
      " |          proto).\n",
      " |        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n",
      " |          the operation will have.\n",
      " |        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n",
      " |          computed).\n",
      " |        compute_device: (Optional.) If True, device functions will be executed\n",
      " |          to compute the device property of the Operation.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if any of the inputs is not a `Tensor`.\n",
      " |        ValueError: if colocation conflicts with existing device assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` object.\n",
      " |  \n",
      " |  device(self, device_name_or_function)\n",
      " |      Returns a context manager that specifies the default device to use.\n",
      " |      \n",
      " |      The `device_name_or_function` argument may either be a device name\n",
      " |      string, a device function, or None:\n",
      " |      \n",
      " |      * If it is a device name string, all operations constructed in\n",
      " |        this context will be assigned to the device with that name, unless\n",
      " |        overridden by a nested `device()` context.\n",
      " |      * If it is a function, it will be treated as a function from\n",
      " |        Operation objects to device name strings, and invoked each time\n",
      " |        a new Operation is created. The Operation will be assigned to\n",
      " |        the device with the returned name.\n",
      " |      * If it is None, all `device()` invocations from the enclosing context\n",
      " |        will be ignored.\n",
      " |      \n",
      " |      For information about the valid syntax of device name strings, see\n",
      " |      the documentation in\n",
      " |      [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.device('/device:GPU:0'):\n",
      " |        # All operations constructed in this context will be placed\n",
      " |        # on GPU 0.\n",
      " |        with g.device(None):\n",
      " |          # All operations constructed in this context will have no\n",
      " |          # assigned device.\n",
      " |      \n",
      " |      # Defines a function from `Operation` to device string.\n",
      " |      def matmul_on_gpu(n):\n",
      " |        if n.type == \"MatMul\":\n",
      " |          return \"/device:GPU:0\"\n",
      " |        else:\n",
      " |          return \"/cpu:0\"\n",
      " |      \n",
      " |      with g.device(matmul_on_gpu):\n",
      " |        # All operations of type \"MatMul\" constructed in this context\n",
      " |        # will be placed on GPU 0; all other operations will be placed\n",
      " |        # on CPU 0.\n",
      " |      ```\n",
      " |      \n",
      " |      **N.B.** The device scope may be overridden by op wrappers or\n",
      " |      other library code. For example, a variable assignment op\n",
      " |      `v.assign()` must be colocated with the `tf.Variable` `v`, and\n",
      " |      incompatible device scopes will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |        device_name_or_function: The device name or function to use in\n",
      " |          the context.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the default device to use for newly\n",
      " |        created ops.\n",
      " |  \n",
      " |  finalize(self)\n",
      " |      Finalizes this graph, making it read-only.\n",
      " |      \n",
      " |      After calling `g.finalize()`, no new operations can be added to\n",
      " |      `g`.  This method is used to ensure that no operations are added\n",
      " |      to a graph when it is shared between multiple threads, for example\n",
      " |      when using a `tf.train.QueueRunner`.\n",
      " |  \n",
      " |  get_all_collection_keys(self)\n",
      " |      Returns a list of collections used in this graph.\n",
      " |  \n",
      " |  get_collection(self, name, scope=None)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      This is different from `get_collection_ref()` which always returns the\n",
      " |      actual collection list if it exists in that it returns a new list each time\n",
      " |      it is called.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        scope: (Optional.) A string. If supplied, the resulting list is filtered\n",
      " |          to include only items whose `name` attribute matches `scope` using\n",
      " |          `re.match`. Items without a `name` attribute are never returned if a\n",
      " |          scope is supplied. The choice of `re.match` means that a `scope` without\n",
      " |          special tokens filters by prefix.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or\n",
      " |        an empty list if no value has been added to that collection. The\n",
      " |        list contains the values in the order under which they were\n",
      " |        collected.\n",
      " |  \n",
      " |  get_collection_ref(self, name)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      If the collection exists, this returns the list itself, which can\n",
      " |      be modified in place to change the collection.  If the collection does\n",
      " |      not exist, it is created as an empty list and the list is returned.\n",
      " |      \n",
      " |      This is different from `get_collection()` which always returns a copy of\n",
      " |      the collection list if it exists and never creates an empty collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or an empty\n",
      " |        list if no value has been added to that collection.\n",
      " |  \n",
      " |  get_name_scope(self)\n",
      " |      Returns the current name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.name_scope('scope1'):\n",
      " |        with tf.name_scope('scope2'):\n",
      " |          print(tf.get_default_graph().get_name_scope())\n",
      " |      ```\n",
      " |      would print the string `scope1/scope2`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string representing the current name scope.\n",
      " |  \n",
      " |  get_operation_by_name(self, name)\n",
      " |      Returns the `Operation` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Operation` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Operation` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to an operation in this graph.\n",
      " |  \n",
      " |  get_operations(self)\n",
      " |      Return the list of operations in the graph.\n",
      " |      \n",
      " |      You can modify the operations in place, but modifications\n",
      " |      to the list such as inserts/delete have no effect on the\n",
      " |      list of operations known to the graph.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of Operations.\n",
      " |  \n",
      " |  get_tensor_by_name(self, name)\n",
      " |      Returns the `Tensor` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Tensor` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to a tensor in this graph.\n",
      " |  \n",
      " |  gradient_override_map(self, op_type_map)\n",
      " |      EXPERIMENTAL: A context manager for overriding gradient functions.\n",
      " |      \n",
      " |      This context manager can be used to override the gradient function\n",
      " |      that will be used for ops within the scope of the context.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tf.RegisterGradient(\"CustomSquare\")\n",
      " |      def _custom_square_grad(op, grad):\n",
      " |        # ...\n",
      " |      \n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n",
      " |        with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n",
      " |          s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n",
      " |                                # gradient of s_2.\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type_map: A dictionary mapping op type strings to alternative op\n",
      " |          type strings.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that sets the alternative op type to be used for one\n",
      " |        or more ops created in that context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `op_type_map` is not a dictionary mapping strings to\n",
      " |          strings.\n",
      " |  \n",
      " |  is_feedable(self, tensor)\n",
      " |      Returns `True` if and only if `tensor` is feedable.\n",
      " |  \n",
      " |  is_fetchable(self, tensor_or_op)\n",
      " |      Returns `True` if and only if `tensor_or_op` is fetchable.\n",
      " |  \n",
      " |  name_scope(self, name)\n",
      " |      Returns a context manager that creates hierarchical names for operations.\n",
      " |      \n",
      " |      A graph maintains a stack of name scopes. A `with name_scope(...):`\n",
      " |      statement pushes a new name onto the stack for the lifetime of the context.\n",
      " |      \n",
      " |      The `name` argument will be interpreted as follows:\n",
      " |      \n",
      " |      * A string (not ending with '/') will create a new name scope, in which\n",
      " |        `name` is appended to the prefix of all operations created in the\n",
      " |        context. If `name` has been used before, it will be made unique by\n",
      " |        calling `self.unique_name(name)`.\n",
      " |      * A scope previously captured from a `with g.name_scope(...) as\n",
      " |        scope:` statement will be treated as an \"absolute\" name scope, which\n",
      " |        makes it possible to re-enter existing scopes.\n",
      " |      * A value of `None` or the empty string will reset the current name scope\n",
      " |        to the top-level (empty) name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0, name=\"c\")\n",
      " |        assert c.op.name == \"c\"\n",
      " |        c_1 = tf.constant(6.0, name=\"c\")\n",
      " |        assert c_1.op.name == \"c_1\"\n",
      " |      \n",
      " |        # Creates a scope called \"nested\"\n",
      " |        with g.name_scope(\"nested\") as scope:\n",
      " |          nested_c = tf.constant(10.0, name=\"c\")\n",
      " |          assert nested_c.op.name == \"nested/c\"\n",
      " |      \n",
      " |          # Creates a nested scope called \"inner\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_c = tf.constant(20.0, name=\"c\")\n",
      " |            assert nested_inner_c.op.name == \"nested/inner/c\"\n",
      " |      \n",
      " |          # Create a nested scope called \"inner_1\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_1_c = tf.constant(30.0, name=\"c\")\n",
      " |            assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n",
      " |      \n",
      " |            # Treats `scope` as an absolute name scope, and\n",
      " |            # switches to the \"nested/\" scope.\n",
      " |            with g.name_scope(scope):\n",
      " |              nested_d = tf.constant(40.0, name=\"d\")\n",
      " |              assert nested_d.op.name == \"nested/d\"\n",
      " |      \n",
      " |              with g.name_scope(\"\"):\n",
      " |                e = tf.constant(50.0, name=\"e\")\n",
      " |                assert e.op.name == \"e\"\n",
      " |      ```\n",
      " |      \n",
      " |      The name of the scope itself can be captured by `with\n",
      " |      g.name_scope(...) as scope:`, which stores the name of the scope\n",
      " |      in the variable `scope`. This value can be used to name an\n",
      " |      operation that represents the overall result of executing the ops\n",
      " |      in a scope. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.constant(...)\n",
      " |      with g.name_scope('my_layer') as scope:\n",
      " |        weights = tf.Variable(..., name=\"weights\")\n",
      " |        biases = tf.Variable(..., name=\"biases\")\n",
      " |        affine = tf.matmul(inputs, weights) + biases\n",
      " |        output = tf.nn.relu(affine, name=scope)\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This constructor validates the given `name`. Valid scope\n",
      " |      names match one of the following regular expressions:\n",
      " |      \n",
      " |          [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]* (for scopes at the root)\n",
      " |          [A-Za-z0-9_.\\\\-/]* (for other scopes)\n",
      " |      \n",
      " |      Args:\n",
      " |        name: A name for the scope.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that installs `name` as a new name scope.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `name` is not a valid scope name, according to the rules\n",
      " |          above.\n",
      " |  \n",
      " |  prevent_feeding(self, tensor)\n",
      " |      Marks the given `tensor` as unfeedable in this graph.\n",
      " |  \n",
      " |  prevent_fetching(self, op)\n",
      " |      Marks the given `op` as unfetchable in this graph.\n",
      " |  \n",
      " |  switch_to_thread_local(self)\n",
      " |      Make device, colocation and dependencies stacks thread-local.\n",
      " |      \n",
      " |      Device, colocation and dependencies stacks are not thread-local be default.\n",
      " |      If multiple threads access them, then the state is shared.  This means that\n",
      " |      one thread may affect the behavior of another thread.\n",
      " |      \n",
      " |      After this method is called, the stacks become thread-local.  If multiple\n",
      " |      threads access them, then the state is not shared.  Each thread uses its own\n",
      " |      value; a thread doesn't affect other threads by mutating such a stack.\n",
      " |      \n",
      " |      The initial value for every thread's stack is set to the current value\n",
      " |      of the stack when `switch_to_thread_local()` was first called.\n",
      " |  \n",
      " |  unique_name(self, name, mark_as_used=True)\n",
      " |      Return a unique operation name for `name`.\n",
      " |      \n",
      " |      Note: You rarely need to call `unique_name()` directly.  Most of\n",
      " |      the time you just need to create `with g.name_scope()` blocks to\n",
      " |      generate structured names.\n",
      " |      \n",
      " |      `unique_name` is used to generate structured names, separated by\n",
      " |      `\"/\"`, to help identify operations when debugging a graph.\n",
      " |      Operation names are displayed in error messages reported by the\n",
      " |      TensorFlow runtime, and in various visualization tools such as\n",
      " |      TensorBoard.\n",
      " |      \n",
      " |      If `mark_as_used` is set to `True`, which is the default, a new\n",
      " |      unique name is created and marked as in use. If it's set to `False`,\n",
      " |      the unique name is returned without actually being marked as used.\n",
      " |      This is useful when the caller simply wants to know what the name\n",
      " |      to be created will be.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name for an operation.\n",
      " |        mark_as_used: Whether to mark this name as being used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string to be passed to `create_op()` that will be used\n",
      " |        to name the operation being created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  building_function\n",
      " |      Returns True iff this graph represents a function.\n",
      " |  \n",
      " |  collections\n",
      " |      Returns the names of the collections known to this graph.\n",
      " |  \n",
      " |  finalized\n",
      " |      True if this graph has been finalized.\n",
      " |  \n",
      " |  graph_def_versions\n",
      " |      The GraphDef version information of this graph.\n",
      " |      \n",
      " |      For details on the meaning of each version, see\n",
      " |      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VersionDef`.\n",
      " |  \n",
      " |  seed\n",
      " |      The graph-level random seed of this graph.\n",
      " |  \n",
      " |  version\n",
      " |      Returns a version number that increases as ops are added to the graph.\n",
      " |      \n",
      " |      Note that this is unrelated to the\n",
      " |      `tf.Graph.graph_def_versions`.\n",
      " |      \n",
      " |      Returns:\n",
      " |         An integer version that increases as ops are added to the graph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versions {\n",
       "  producer: 27\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function enable_eager_execution in module tensorflow.python.framework.ops:\n",
      "\n",
      "enable_eager_execution(config=None, device_policy=None, execution_mode=None)\n",
      "    Enables eager execution for the lifetime of this program.\n",
      "    \n",
      "    Eager execution provides an imperative interface to TensorFlow. With eager\n",
      "    execution enabled, TensorFlow functions execute operations immediately (as\n",
      "    opposed to adding to a graph to be executed later in a `tf.Session`) and\n",
      "    return concrete values (as opposed to symbolic references to a node in a\n",
      "    computational graph).\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    tf.enable_eager_execution()\n",
      "    \n",
      "    # After eager execution is enabled, operations are executed as they are\n",
      "    # defined and Tensor objects hold concrete values, which can be accessed as\n",
      "    # numpy.ndarray`s through the numpy() method.\n",
      "    assert tf.multiply(6, 7).numpy() == 42\n",
      "    ```\n",
      "    \n",
      "    Eager execution cannot be enabled after TensorFlow APIs have been used to\n",
      "    create or execute graphs. It is typically recommended to invoke this function\n",
      "    at program startup and not in a library (as most libraries should be usable\n",
      "    both with and without eager execution).\n",
      "    \n",
      "    Args:\n",
      "      config: (Optional.) A `tf.ConfigProto` to use to configure the environment\n",
      "        in which operations are executed. Note that `tf.ConfigProto` is also\n",
      "        used to configure graph execution (via `tf.Session`) and many options\n",
      "        within `tf.ConfigProto` are not implemented (or are irrelevant) when\n",
      "        eager execution is enabled.\n",
      "      device_policy: (Optional.) Policy controlling how operations requiring\n",
      "        inputs on a specific device (e.g., a GPU 0) handle inputs on a different\n",
      "        device  (e.g. GPU 1 or CPU). When set to None, an appropriate value will be\n",
      "        picked automatically. The value picked may change between TensorFlow\n",
      "        releases.\n",
      "        Valid values:\n",
      "        - tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT: raises an error if the\n",
      "          placement is not correct.\n",
      "        - tf.contrib.eager.DEVICE_PLACEMENT_WARN: copies the tensors which are not\n",
      "          on the right device but logs a warning.\n",
      "        - tf.contrib.eager.DEVICE_PLACEMENT_SILENT: silently copies the tensors.\n",
      "          Note that this may hide performance problems as there is no notification\n",
      "          provided when operations are blocked on the tensor being copied between\n",
      "          devices.\n",
      "        - tf.contrib.eager.DEVICE_PLACEMENT_SILENT_FOR_INT32: silently copies\n",
      "          int32 tensors, raising errors on the other ones.\n",
      "      execution_mode: (Optional.) Policy controlling how operations dispatched are\n",
      "        actually executed. When set to None, an appropriate value will be picked\n",
      "        automatically. The value picked may change between TensorFlow releases.\n",
      "        Valid values:\n",
      "        - tf.contrib.eager.SYNC: executes each operation synchronously.\n",
      "        - tf.contrib.eager.ASYNC: executes each operation asynchronously. These\n",
      "          operations may return \"non-ready\" handles.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If eager execution is enabled after creating/executing a\n",
      "       TensorFlow graph, or if options provided conflict with a previous call\n",
      "       to this function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.enable_eager_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Graph in module tensorflow.python.framework.ops:\n",
      "\n",
      "class Graph(builtins.object)\n",
      " |  A TensorFlow computation, represented as a dataflow graph.\n",
      " |  \n",
      " |  A `Graph` contains a set of\n",
      " |  `tf.Operation` objects,\n",
      " |  which represent units of computation; and\n",
      " |  `tf.Tensor` objects, which represent\n",
      " |  the units of data that flow between operations.\n",
      " |  \n",
      " |  A default `Graph` is always registered, and accessible by calling\n",
      " |  `tf.get_default_graph`.\n",
      " |  To add an operation to the default graph, simply call one of the functions\n",
      " |  that defines a new `Operation`:\n",
      " |  \n",
      " |  ```python\n",
      " |  c = tf.constant(4.0)\n",
      " |  assert c.graph is tf.get_default_graph()\n",
      " |  ```\n",
      " |  \n",
      " |  Another typical usage involves the\n",
      " |  `tf.Graph.as_default`\n",
      " |  context manager, which overrides the current default graph for the\n",
      " |  lifetime of the context:\n",
      " |  \n",
      " |  ```python\n",
      " |  g = tf.Graph()\n",
      " |  with g.as_default():\n",
      " |    # Define operations and tensors in `g`.\n",
      " |    c = tf.constant(30.0)\n",
      " |    assert c.graph is g\n",
      " |  ```\n",
      " |  \n",
      " |  Important note: This class *is not* thread-safe for graph construction. All\n",
      " |  operations should be created from a single thread, or external\n",
      " |  synchronization must be provided. Unless otherwise specified, all methods\n",
      " |  are not thread-safe.\n",
      " |  \n",
      " |  A `Graph` instance supports an arbitrary number of \"collections\"\n",
      " |  that are identified by name. For convenience when building a large\n",
      " |  graph, collections can store groups of related objects: for\n",
      " |  example, the `tf.Variable` uses a collection (named\n",
      " |  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n",
      " |  all variables that are created during the construction of a graph. The caller\n",
      " |  may define additional collections by specifying a new name.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Creates a new, empty Graph.\n",
      " |  \n",
      " |  add_to_collection(self, name, value)\n",
      " |      Stores `value` in the collection with the given `name`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collection.\n",
      " |  \n",
      " |  add_to_collections(self, names, value)\n",
      " |      Stores `value` in the collections given by `names`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times. This function makes sure that duplicates in\n",
      " |      `names` are ignored, but it will not check for pre-existing membership of\n",
      " |      `value` in any of the collections in `names`.\n",
      " |      \n",
      " |      `names` can be any iterable, but if `names` is a string, it is treated as a\n",
      " |      single collection name.\n",
      " |      \n",
      " |      Args:\n",
      " |        names: The keys for the collections to add to. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collections.\n",
      " |  \n",
      " |  as_default(self)\n",
      " |      Returns a context manager that makes this `Graph` the default graph.\n",
      " |      \n",
      " |      This method should be used if you want to create multiple graphs\n",
      " |      in the same process. For convenience, a global default graph is\n",
      " |      provided, and all ops will be added to this graph if you do not\n",
      " |      create a new graph explicitly.\n",
      " |      \n",
      " |      Use this method with the `with` keyword to specify that ops created within\n",
      " |      the scope of a block should be added to this graph. In this case, once\n",
      " |      the scope of the `with` is exited, the previous default graph is set again\n",
      " |      as default. There is a stack, so it's ok to have multiple nested levels\n",
      " |      of `as_default` calls.\n",
      " |      \n",
      " |      The default graph is a property of the current thread. If you\n",
      " |      create a new thread, and wish to use the default graph in that\n",
      " |      thread, you must explicitly add a `with g.as_default():` in that\n",
      " |      thread's function.\n",
      " |      \n",
      " |      The following code examples are equivalent:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 1. Using Graph.as_default():\n",
      " |      g = tf.Graph()\n",
      " |      with g.as_default():\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      \n",
      " |      # 2. Constructing and making default:\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      ```\n",
      " |      \n",
      " |      If eager execution is enabled ops created under this context manager will be\n",
      " |      added to the graph instead of executed eagerly.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for using this graph as the default graph.\n",
      " |  \n",
      " |  as_graph_def(self, from_version=None, add_shapes=False)\n",
      " |      Returns a serialized `GraphDef` representation of this graph.\n",
      " |      \n",
      " |      The serialized `GraphDef` can be imported into another `Graph`\n",
      " |      (using `tf.import_graph_def`) or used with the\n",
      " |      [C++ Session API](../../api_docs/cc/index.md).\n",
      " |      \n",
      " |      This method is thread-safe.\n",
      " |      \n",
      " |      Args:\n",
      " |        from_version: Optional.  If this is set, returns a `GraphDef`\n",
      " |          containing only the nodes that were added to this graph since\n",
      " |          its `version` property had the given value.\n",
      " |        add_shapes: If true, adds an \"_output_shapes\" list attr to each\n",
      " |          node with the inferred shapes of each of its outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A\n",
      " |        [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)\n",
      " |        protocol buffer.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the `graph_def` would be too large.\n",
      " |  \n",
      " |  as_graph_element(self, obj, allow_tensor=True, allow_operation=True)\n",
      " |      Returns the object referred to by `obj`, as an `Operation` or `Tensor`.\n",
      " |      \n",
      " |      This function validates that `obj` represents an element of this\n",
      " |      graph, and gives an informative error message if it is not.\n",
      " |      \n",
      " |      This function is the canonical way to get/validate an object of\n",
      " |      one of the allowed types from an external argument reference in the\n",
      " |      Session API.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        obj: A `Tensor`, an `Operation`, or the name of a tensor or operation.\n",
      " |          Can also be any object with an `_as_graph_element()` method that returns\n",
      " |          a value of one of these types.\n",
      " |        allow_tensor: If true, `obj` may refer to a `Tensor`.\n",
      " |        allow_operation: If true, `obj` may refer to an `Operation`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` or `Operation` in the Graph corresponding to `obj`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `obj` is not a type we support attempting to convert\n",
      " |          to types.\n",
      " |        ValueError: If `obj` is of an appropriate type but invalid. For\n",
      " |          example, an invalid string.\n",
      " |        KeyError: If `obj` is not an object in the graph.\n",
      " |  \n",
      " |  clear_collection(self, name)\n",
      " |      Clears all values in a collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class contains many\n",
      " |          standard names for collections.\n",
      " |  \n",
      " |  colocate_with(self, op, ignore_existing=False)\n",
      " |      Returns a context manager that specifies an op to colocate with.\n",
      " |      \n",
      " |      Note: this function is not for public use, only for internal libraries.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = tf.Variable([1.0])\n",
      " |      with g.colocate_with(a):\n",
      " |        b = tf.constant(1.0)\n",
      " |        c = tf.add(a, b)\n",
      " |      ```\n",
      " |      \n",
      " |      `b` and `c` will always be colocated with `a`, no matter where `a`\n",
      " |      is eventually placed.\n",
      " |      \n",
      " |      **NOTE** Using a colocation scope resets any existing device constraints.\n",
      " |      \n",
      " |      If `op` is `None` then `ignore_existing` must be `True` and the new\n",
      " |      scope resets all colocation and device constraints.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: The op to colocate all created ops with, or `None`.\n",
      " |        ignore_existing: If true, only applies colocation of this op within\n",
      " |          the context, rather than applying all colocation properties\n",
      " |          on the stack.  If `op` is `None`, this value must be `True`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if op is None but ignore_existing is False.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the op with which to colocate\n",
      " |        newly created ops.\n",
      " |  \n",
      " |  container(self, container_name)\n",
      " |      Returns a context manager that specifies the resource container to use.\n",
      " |      \n",
      " |      Stateful operations, such as variables and queues, can maintain their\n",
      " |      states on devices so that they can be shared by multiple processes.\n",
      " |      A resource container is a string name under which these stateful\n",
      " |      operations are tracked. These resources can be released or cleared\n",
      " |      with `tf.Session.reset()`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.container('experiment0'):\n",
      " |        # All stateful Operations constructed in this context will be placed\n",
      " |        # in resource container \"experiment0\".\n",
      " |        v1 = tf.Variable([1.0])\n",
      " |        v2 = tf.Variable([2.0])\n",
      " |        with g.container(\"experiment1\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # placed in resource container \"experiment1\".\n",
      " |          v3 = tf.Variable([3.0])\n",
      " |          q1 = tf.FIFOQueue(10, tf.float32)\n",
      " |        # All stateful Operations constructed in this context will be\n",
      " |        # be created in the \"experiment0\".\n",
      " |        v4 = tf.Variable([4.0])\n",
      " |        q1 = tf.FIFOQueue(20, tf.float32)\n",
      " |        with g.container(\"\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # be placed in the default resource container.\n",
      " |          v5 = tf.Variable([5.0])\n",
      " |          q3 = tf.FIFOQueue(30, tf.float32)\n",
      " |      \n",
      " |      # Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n",
      " |      # will become undefined (such as uninitialized).\n",
      " |      tf.Session.reset(target, [\"experiment0\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        container_name: container name string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for defining resource containers for stateful ops,\n",
      " |          yields the container name.\n",
      " |  \n",
      " |  control_dependencies(self, control_inputs)\n",
      " |      Returns a context manager that specifies control dependencies.\n",
      " |      \n",
      " |      Use with the `with` keyword to specify that all operations constructed\n",
      " |      within the context should have control dependencies on\n",
      " |      `control_inputs`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b, c]):\n",
      " |        # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n",
      " |        d = ...\n",
      " |        e = ...\n",
      " |      ```\n",
      " |      \n",
      " |      Multiple calls to `control_dependencies()` can be nested, and in\n",
      " |      that case a new `Operation` will have control dependencies on the union\n",
      " |      of `control_inputs` from all active contexts.\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies([c, d]):\n",
      " |          # Ops constructed here run after `a`, `b`, `c`, and `d`.\n",
      " |      ```\n",
      " |      \n",
      " |      You can pass None to clear the control dependencies:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies(None):\n",
      " |          # Ops constructed here run normally, not waiting for either `a` or `b`.\n",
      " |          with g.control_dependencies([c, d]):\n",
      " |            # Ops constructed here run after `c` and `d`, also not waiting\n",
      " |            # for either `a` or `b`.\n",
      " |      ```\n",
      " |      \n",
      " |      *N.B.* The control dependencies context applies *only* to ops that\n",
      " |      are constructed within the context. Merely using an op or tensor\n",
      " |      in the context does not add a control dependency. The following\n",
      " |      example illustrates this point:\n",
      " |      \n",
      " |      ```python\n",
      " |      # WRONG\n",
      " |      def my_func(pred, tensor):\n",
      " |        t = tf.matmul(tensor, tensor)\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created outside the context, so no control\n",
      " |          # dependency will be added.\n",
      " |          return t\n",
      " |      \n",
      " |      # RIGHT\n",
      " |      def my_func(pred, tensor):\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created in the context, so a control dependency\n",
      " |          # will be added.\n",
      " |          return tf.matmul(tensor, tensor)\n",
      " |      ```\n",
      " |      \n",
      " |      Also note that though execution of ops created under this scope will trigger\n",
      " |      execution of the dependencies, the ops created under this scope might still\n",
      " |      be pruned from a normal tensorflow graph. For example, in the following\n",
      " |      snippet of code the dependencies are never executed:\n",
      " |      \n",
      " |      ```python\n",
      " |        loss = model.loss()\n",
      " |        with tf.control_dependencies(dependencies):\n",
      " |          loss = loss + tf.constant(1)  # note: dependencies ignored in the\n",
      " |                                        # backward pass\n",
      " |        return tf.gradients(loss, model.variables)\n",
      " |      ```\n",
      " |      \n",
      " |      This is because evaluating the gradient graph does not require evaluating\n",
      " |      the constant(1) op created in the forward pass.\n",
      " |      \n",
      " |      Args:\n",
      " |        control_inputs: A list of `Operation` or `Tensor` objects which\n",
      " |          must be executed or computed before running the operations\n",
      " |          defined in the context.  Can also be `None` to clear the control\n",
      " |          dependencies.\n",
      " |      \n",
      " |      Returns:\n",
      " |       A context manager that specifies control dependencies for all\n",
      " |       operations constructed within the context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `control_inputs` is not a list of `Operation` or\n",
      " |          `Tensor` objects.\n",
      " |  \n",
      " |  create_op(self, op_type, inputs, dtypes, input_types=None, name=None, attrs=None, op_def=None, compute_shapes=True, compute_device=True)\n",
      " |      Creates an `Operation` in this graph. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(compute_shapes)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      " |      \n",
      " |      This is a low-level interface for creating an `Operation`. Most\n",
      " |      programs will not call this method directly, and instead use the\n",
      " |      Python op constructors, such as `tf.constant()`, which add ops to\n",
      " |      the default graph.\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type: The `Operation` type to create. This corresponds to the\n",
      " |          `OpDef.name` field for the proto that defines the operation.\n",
      " |        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n",
      " |        dtypes: A list of `DType` objects that will be the types of the tensors\n",
      " |          that the operation produces.\n",
      " |        input_types: (Optional.) A list of `DType`s that will be the types of\n",
      " |          the tensors that the operation consumes. By default, uses the base\n",
      " |          `DType` of each input in `inputs`. Operations that expect\n",
      " |          reference-typed inputs must specify `input_types` explicitly.\n",
      " |        name: (Optional.) A string name for the operation. If not specified, a\n",
      " |          name is generated based on `op_type`.\n",
      " |        attrs: (Optional.) A dictionary where the key is the attribute name (a\n",
      " |          string) and the value is the respective `attr` attribute of the\n",
      " |          `NodeDef` proto that will represent the operation (an `AttrValue`\n",
      " |          proto).\n",
      " |        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n",
      " |          the operation will have.\n",
      " |        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n",
      " |          computed).\n",
      " |        compute_device: (Optional.) If True, device functions will be executed\n",
      " |          to compute the device property of the Operation.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if any of the inputs is not a `Tensor`.\n",
      " |        ValueError: if colocation conflicts with existing device assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` object.\n",
      " |  \n",
      " |  device(self, device_name_or_function)\n",
      " |      Returns a context manager that specifies the default device to use.\n",
      " |      \n",
      " |      The `device_name_or_function` argument may either be a device name\n",
      " |      string, a device function, or None:\n",
      " |      \n",
      " |      * If it is a device name string, all operations constructed in\n",
      " |        this context will be assigned to the device with that name, unless\n",
      " |        overridden by a nested `device()` context.\n",
      " |      * If it is a function, it will be treated as a function from\n",
      " |        Operation objects to device name strings, and invoked each time\n",
      " |        a new Operation is created. The Operation will be assigned to\n",
      " |        the device with the returned name.\n",
      " |      * If it is None, all `device()` invocations from the enclosing context\n",
      " |        will be ignored.\n",
      " |      \n",
      " |      For information about the valid syntax of device name strings, see\n",
      " |      the documentation in\n",
      " |      [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.device('/device:GPU:0'):\n",
      " |        # All operations constructed in this context will be placed\n",
      " |        # on GPU 0.\n",
      " |        with g.device(None):\n",
      " |          # All operations constructed in this context will have no\n",
      " |          # assigned device.\n",
      " |      \n",
      " |      # Defines a function from `Operation` to device string.\n",
      " |      def matmul_on_gpu(n):\n",
      " |        if n.type == \"MatMul\":\n",
      " |          return \"/device:GPU:0\"\n",
      " |        else:\n",
      " |          return \"/cpu:0\"\n",
      " |      \n",
      " |      with g.device(matmul_on_gpu):\n",
      " |        # All operations of type \"MatMul\" constructed in this context\n",
      " |        # will be placed on GPU 0; all other operations will be placed\n",
      " |        # on CPU 0.\n",
      " |      ```\n",
      " |      \n",
      " |      **N.B.** The device scope may be overridden by op wrappers or\n",
      " |      other library code. For example, a variable assignment op\n",
      " |      `v.assign()` must be colocated with the `tf.Variable` `v`, and\n",
      " |      incompatible device scopes will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |        device_name_or_function: The device name or function to use in\n",
      " |          the context.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the default device to use for newly\n",
      " |        created ops.\n",
      " |  \n",
      " |  finalize(self)\n",
      " |      Finalizes this graph, making it read-only.\n",
      " |      \n",
      " |      After calling `g.finalize()`, no new operations can be added to\n",
      " |      `g`.  This method is used to ensure that no operations are added\n",
      " |      to a graph when it is shared between multiple threads, for example\n",
      " |      when using a `tf.train.QueueRunner`.\n",
      " |  \n",
      " |  get_all_collection_keys(self)\n",
      " |      Returns a list of collections used in this graph.\n",
      " |  \n",
      " |  get_collection(self, name, scope=None)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      This is different from `get_collection_ref()` which always returns the\n",
      " |      actual collection list if it exists in that it returns a new list each time\n",
      " |      it is called.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        scope: (Optional.) A string. If supplied, the resulting list is filtered\n",
      " |          to include only items whose `name` attribute matches `scope` using\n",
      " |          `re.match`. Items without a `name` attribute are never returned if a\n",
      " |          scope is supplied. The choice of `re.match` means that a `scope` without\n",
      " |          special tokens filters by prefix.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or\n",
      " |        an empty list if no value has been added to that collection. The\n",
      " |        list contains the values in the order under which they were\n",
      " |        collected.\n",
      " |  \n",
      " |  get_collection_ref(self, name)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      If the collection exists, this returns the list itself, which can\n",
      " |      be modified in place to change the collection.  If the collection does\n",
      " |      not exist, it is created as an empty list and the list is returned.\n",
      " |      \n",
      " |      This is different from `get_collection()` which always returns a copy of\n",
      " |      the collection list if it exists and never creates an empty collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or an empty\n",
      " |        list if no value has been added to that collection.\n",
      " |  \n",
      " |  get_name_scope(self)\n",
      " |      Returns the current name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.name_scope('scope1'):\n",
      " |        with tf.name_scope('scope2'):\n",
      " |          print(tf.get_default_graph().get_name_scope())\n",
      " |      ```\n",
      " |      would print the string `scope1/scope2`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string representing the current name scope.\n",
      " |  \n",
      " |  get_operation_by_name(self, name)\n",
      " |      Returns the `Operation` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Operation` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Operation` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to an operation in this graph.\n",
      " |  \n",
      " |  get_operations(self)\n",
      " |      Return the list of operations in the graph.\n",
      " |      \n",
      " |      You can modify the operations in place, but modifications\n",
      " |      to the list such as inserts/delete have no effect on the\n",
      " |      list of operations known to the graph.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of Operations.\n",
      " |  \n",
      " |  get_tensor_by_name(self, name)\n",
      " |      Returns the `Tensor` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Tensor` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to a tensor in this graph.\n",
      " |  \n",
      " |  gradient_override_map(self, op_type_map)\n",
      " |      EXPERIMENTAL: A context manager for overriding gradient functions.\n",
      " |      \n",
      " |      This context manager can be used to override the gradient function\n",
      " |      that will be used for ops within the scope of the context.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tf.RegisterGradient(\"CustomSquare\")\n",
      " |      def _custom_square_grad(op, grad):\n",
      " |        # ...\n",
      " |      \n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n",
      " |        with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n",
      " |          s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n",
      " |                                # gradient of s_2.\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type_map: A dictionary mapping op type strings to alternative op\n",
      " |          type strings.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that sets the alternative op type to be used for one\n",
      " |        or more ops created in that context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `op_type_map` is not a dictionary mapping strings to\n",
      " |          strings.\n",
      " |  \n",
      " |  is_feedable(self, tensor)\n",
      " |      Returns `True` if and only if `tensor` is feedable.\n",
      " |  \n",
      " |  is_fetchable(self, tensor_or_op)\n",
      " |      Returns `True` if and only if `tensor_or_op` is fetchable.\n",
      " |  \n",
      " |  name_scope(self, name)\n",
      " |      Returns a context manager that creates hierarchical names for operations.\n",
      " |      \n",
      " |      A graph maintains a stack of name scopes. A `with name_scope(...):`\n",
      " |      statement pushes a new name onto the stack for the lifetime of the context.\n",
      " |      \n",
      " |      The `name` argument will be interpreted as follows:\n",
      " |      \n",
      " |      * A string (not ending with '/') will create a new name scope, in which\n",
      " |        `name` is appended to the prefix of all operations created in the\n",
      " |        context. If `name` has been used before, it will be made unique by\n",
      " |        calling `self.unique_name(name)`.\n",
      " |      * A scope previously captured from a `with g.name_scope(...) as\n",
      " |        scope:` statement will be treated as an \"absolute\" name scope, which\n",
      " |        makes it possible to re-enter existing scopes.\n",
      " |      * A value of `None` or the empty string will reset the current name scope\n",
      " |        to the top-level (empty) name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0, name=\"c\")\n",
      " |        assert c.op.name == \"c\"\n",
      " |        c_1 = tf.constant(6.0, name=\"c\")\n",
      " |        assert c_1.op.name == \"c_1\"\n",
      " |      \n",
      " |        # Creates a scope called \"nested\"\n",
      " |        with g.name_scope(\"nested\") as scope:\n",
      " |          nested_c = tf.constant(10.0, name=\"c\")\n",
      " |          assert nested_c.op.name == \"nested/c\"\n",
      " |      \n",
      " |          # Creates a nested scope called \"inner\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_c = tf.constant(20.0, name=\"c\")\n",
      " |            assert nested_inner_c.op.name == \"nested/inner/c\"\n",
      " |      \n",
      " |          # Create a nested scope called \"inner_1\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_1_c = tf.constant(30.0, name=\"c\")\n",
      " |            assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n",
      " |      \n",
      " |            # Treats `scope` as an absolute name scope, and\n",
      " |            # switches to the \"nested/\" scope.\n",
      " |            with g.name_scope(scope):\n",
      " |              nested_d = tf.constant(40.0, name=\"d\")\n",
      " |              assert nested_d.op.name == \"nested/d\"\n",
      " |      \n",
      " |              with g.name_scope(\"\"):\n",
      " |                e = tf.constant(50.0, name=\"e\")\n",
      " |                assert e.op.name == \"e\"\n",
      " |      ```\n",
      " |      \n",
      " |      The name of the scope itself can be captured by `with\n",
      " |      g.name_scope(...) as scope:`, which stores the name of the scope\n",
      " |      in the variable `scope`. This value can be used to name an\n",
      " |      operation that represents the overall result of executing the ops\n",
      " |      in a scope. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.constant(...)\n",
      " |      with g.name_scope('my_layer') as scope:\n",
      " |        weights = tf.Variable(..., name=\"weights\")\n",
      " |        biases = tf.Variable(..., name=\"biases\")\n",
      " |        affine = tf.matmul(inputs, weights) + biases\n",
      " |        output = tf.nn.relu(affine, name=scope)\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This constructor validates the given `name`. Valid scope\n",
      " |      names match one of the following regular expressions:\n",
      " |      \n",
      " |          [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]* (for scopes at the root)\n",
      " |          [A-Za-z0-9_.\\\\-/]* (for other scopes)\n",
      " |      \n",
      " |      Args:\n",
      " |        name: A name for the scope.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that installs `name` as a new name scope.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `name` is not a valid scope name, according to the rules\n",
      " |          above.\n",
      " |  \n",
      " |  prevent_feeding(self, tensor)\n",
      " |      Marks the given `tensor` as unfeedable in this graph.\n",
      " |  \n",
      " |  prevent_fetching(self, op)\n",
      " |      Marks the given `op` as unfetchable in this graph.\n",
      " |  \n",
      " |  switch_to_thread_local(self)\n",
      " |      Make device, colocation and dependencies stacks thread-local.\n",
      " |      \n",
      " |      Device, colocation and dependencies stacks are not thread-local be default.\n",
      " |      If multiple threads access them, then the state is shared.  This means that\n",
      " |      one thread may affect the behavior of another thread.\n",
      " |      \n",
      " |      After this method is called, the stacks become thread-local.  If multiple\n",
      " |      threads access them, then the state is not shared.  Each thread uses its own\n",
      " |      value; a thread doesn't affect other threads by mutating such a stack.\n",
      " |      \n",
      " |      The initial value for every thread's stack is set to the current value\n",
      " |      of the stack when `switch_to_thread_local()` was first called.\n",
      " |  \n",
      " |  unique_name(self, name, mark_as_used=True)\n",
      " |      Return a unique operation name for `name`.\n",
      " |      \n",
      " |      Note: You rarely need to call `unique_name()` directly.  Most of\n",
      " |      the time you just need to create `with g.name_scope()` blocks to\n",
      " |      generate structured names.\n",
      " |      \n",
      " |      `unique_name` is used to generate structured names, separated by\n",
      " |      `\"/\"`, to help identify operations when debugging a graph.\n",
      " |      Operation names are displayed in error messages reported by the\n",
      " |      TensorFlow runtime, and in various visualization tools such as\n",
      " |      TensorBoard.\n",
      " |      \n",
      " |      If `mark_as_used` is set to `True`, which is the default, a new\n",
      " |      unique name is created and marked as in use. If it's set to `False`,\n",
      " |      the unique name is returned without actually being marked as used.\n",
      " |      This is useful when the caller simply wants to know what the name\n",
      " |      to be created will be.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name for an operation.\n",
      " |        mark_as_used: Whether to mark this name as being used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string to be passed to `create_op()` that will be used\n",
      " |        to name the operation being created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  building_function\n",
      " |      Returns True iff this graph represents a function.\n",
      " |  \n",
      " |  collections\n",
      " |      Returns the names of the collections known to this graph.\n",
      " |  \n",
      " |  finalized\n",
      " |      True if this graph has been finalized.\n",
      " |  \n",
      " |  graph_def_versions\n",
      " |      The GraphDef version information of this graph.\n",
      " |      \n",
      " |      For details on the meaning of each version, see\n",
      " |      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VersionDef`.\n",
      " |  \n",
      " |  seed\n",
      " |      The graph-level random seed of this graph.\n",
      " |  \n",
      " |  version\n",
      " |      Returns a version number that increases as ops are added to the graph.\n",
      " |      \n",
      " |      Note that this is unrelated to the\n",
      " |      `tf.Graph.graph_def_versions`.\n",
      " |      \n",
      " |      Returns:\n",
      " |         An integer version that increases as ops are added to the graph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
