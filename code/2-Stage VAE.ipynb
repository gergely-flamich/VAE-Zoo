{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Stage VAE\n",
    "\n",
    "Based on [Diagnosing and Enhancing VAE Models](https://arxiv.org/abs/1903.05789) by Dai and Wipf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 16:51:52.187434 140449970063104 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0705 16:51:52.202286 140449970063104 deprecation_wrapper.py:119] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/custom_getters/restore_initializer.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, glob\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfs = tf.contrib.summary\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "import sonnet as snt\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_input_fn(data, batch_size=256, shuffle_samples=5000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(shuffle_samples)\n",
    "    dataset = dataset.map(mnist_parse_fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mnist_parse_fn(data):\n",
    "    return tf.cast(data, tf.float32) / 255.\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=True),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistTwoStageVAE(snt.AbstractModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim=32, \n",
    "                 second_stage_depth=3,\n",
    "                 name=\"mnist_vae\"):\n",
    "\n",
    "        super(MnistTwoStageVAE, self).__init__(name=name)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.second_stage_depth = second_stage_depth\n",
    "        \n",
    "        self.is_training = True\n",
    "        self.use_second_stage = False\n",
    "        \n",
    "        self.first_run = True\n",
    "\n",
    "\n",
    "    @property\n",
    "    def kl_first_stage(self):\n",
    "        \"\"\"\n",
    "        Calculates the KL divergence between the current variational posterior and the prior:\n",
    "\n",
    "        KL[ q(z | theta) || p(z) ]\n",
    "\n",
    "        \"\"\"\n",
    "        self._ensure_is_connected()\n",
    "        \n",
    "        return tfd.kl_divergence(self.latent_posterior, self.latent_prior)\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def kl_second_stage(self):\n",
    "        self._ensure_is_connected()\n",
    "        \n",
    "        return tfd.kl_divergence(self.second_stage_posterior, self.second_stage_prior)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def log_prob_first_stage(self):\n",
    "        return tf.reduce_sum(self._log_prob)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def log_prob_second_stage(self):\n",
    "        return tf.reduce_sum(self._log_prob_second_stage)\n",
    "    \n",
    "    @snt.reuse_variables\n",
    "    def get_first_stage_variables(self):\n",
    "        \n",
    "        all_variables = ()\n",
    "        \n",
    "        # Add all variables from the encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            \n",
    "            if isinstance(layer, snt.AbstractModule):\n",
    "                all_variables += layer.get_all_variables()\n",
    "                \n",
    "        all_variables += self.encoder_loc_head.get_all_variables()  \n",
    "        all_variables += self.encoder_log_scale_head.get_all_variables() \n",
    "        \n",
    "        # Add all variables from the decoder\n",
    "        for layer in self.decoder_levels:    \n",
    "            \n",
    "            if isinstance(layer, snt.AbstractModule):\n",
    "                all_variables += layer.get_all_variables()\n",
    "            \n",
    "        # Add gamma\n",
    "        all_variables += (tf.get_variable(\"log_gamma_x\", [], tf.float32, tf.zeros_initializer()),)\n",
    "        \n",
    "        return all_variables\n",
    "            \n",
    "    \n",
    "    @snt.reuse_variables\n",
    "    def get_second_stage_variables(self):\n",
    "        \n",
    "        all_variables = ()\n",
    "        \n",
    "        # Add all variables from the encoder\n",
    "        for layer in self.second_stage_encoder_layers:\n",
    "            \n",
    "            if isinstance(layer, snt.AbstractModule):\n",
    "                all_variables += layer.get_all_variables()\n",
    "                \n",
    "        all_variables += self.second_stage_loc_head.get_all_variables()  \n",
    "        all_variables += self.second_stage_log_scale_head.get_all_variables() \n",
    "        \n",
    "        # Add all variables from the decoder\n",
    "        for layer in self.second_stage_decoder_layers:    \n",
    "            \n",
    "            if isinstance(layer, snt.AbstractModule):\n",
    "                all_variables += layer.get_all_variables()\n",
    "            \n",
    "        all_variables += self.second_stage_residual_head.get_all_variables()\n",
    "        \n",
    "        # Add gamma\n",
    "        all_variables += (tf.get_variable(\"log_gamma_z\", [], tf.float32, tf.zeros_initializer()),)\n",
    "        \n",
    "        return all_variables\n",
    "\n",
    "    # =====================================================================\n",
    "    # First stage\n",
    "    # =====================================================================\n",
    "\n",
    "    @snt.reuse_variables\n",
    "    def encode_first_stage(self, inputs):\n",
    "        \"\"\"\n",
    "        Builds the encoder part of the VAE, i.e. q(x | theta).\n",
    "        This maps from the input to the latent representation.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        self.encoder_layers = [\n",
    "            snt.Conv2D(output_channels=64,\n",
    "                       kernel_shape=(5, 5),\n",
    "                       stride=2,\n",
    "                       name=\"encoder_conv1\"),\n",
    "            tf.nn.leaky_relu,\n",
    "            snt.Conv2D(output_channels=128,\n",
    "                       kernel_shape=(5, 5),\n",
    "                       stride=2,\n",
    "                       use_bias=False,\n",
    "                       name=\"encoder_conv2\"),\n",
    "            snt.BatchNorm(), #tf.keras.layers.BatchNormalization(),\n",
    "            tf.nn.leaky_relu,\n",
    "            snt.BatchFlatten(),\n",
    "            snt.Linear(output_size=1024,\n",
    "                       use_bias=False,\n",
    "                       name=\"encoder_linear1\"),\n",
    "            snt.BatchNorm(), #tf.keras.layers.BatchNormalization(),\n",
    "            tf.nn.leaky_relu\n",
    "        ]\n",
    "        \n",
    "        self.encoder_loc_head = snt.Linear(output_size=self.latent_dim,\n",
    "                                           name=\"encoder_loc_head\")\n",
    "        self.encoder_log_scale_head = snt.Linear(output_size=self.latent_dim,\n",
    "                                                 name=\"encoder_log_scale_head\")\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        activations = inputs\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            if isinstance(layer, snt.BatchNorm): #tf.keras.layers.BatchNormalization):\n",
    "                activations = layer(activations, is_training=self.is_training and not self.use_second_stage)\n",
    "            else:\n",
    "                activations = layer(activations)\n",
    "            \n",
    "        # Get latent posterior statistics\n",
    "        loc = self.encoder_loc_head(activations)\n",
    "        scale = 1e-6 + tf.nn.softplus(self.encoder_log_scale_head(activations))\n",
    "        \n",
    "        # Create latent posterior\n",
    "        self.latent_posterior = tfd.Normal(loc=loc, scale=scale)\n",
    "        \n",
    "        return self.latent_posterior.sample()\n",
    "\n",
    "\n",
    "    @snt.reuse_variables\n",
    "    def decode_first_stage(self, latent_code):\n",
    "        \"\"\"\n",
    "        Builds the decoder part of the VAE\n",
    "        \"\"\"\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        self.decoder_levels = [self.encoder_loc_head.transpose()]\n",
    "        \n",
    "        for layer in self.encoder_layers[::-1]:\n",
    "            \n",
    "            # Some layers need care for reversing\n",
    "            if isinstance(layer, snt.Transposable):\n",
    "                layer = layer.transpose()\n",
    "                \n",
    "            elif isinstance(layer, snt.BatchFlatten):\n",
    "                layer = BatchReshape((28, 28, 1))\n",
    "           \n",
    "            # Add layer\n",
    "            self.decoder_levels.append(layer)\n",
    "        \n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        # Create prior\n",
    "        self.latent_prior = tfd.Normal(loc=tf.zeros_like(latent_code),\n",
    "                                       scale=tf.ones_like(latent_code))\n",
    "        \n",
    "        activations = latent_code\n",
    "        \n",
    "        for layer in self.decoder_levels:\n",
    "            if isinstance(layer, snt.BatchNorm): #tf.keras.layers.BatchNormalization):\n",
    "                activations = layer(activations, is_training=self.is_training and not self.use_second_stage)\n",
    "            else:\n",
    "                activations = layer(activations)\n",
    "        \n",
    "        return tf.nn.sigmoid(activations)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Second stage\n",
    "    # =====================================================================\n",
    "    \n",
    "    @snt.reuse_variables\n",
    "    def encode_second_stage(self, inputs):\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        self.second_stage_encoder_layers = []\n",
    "        \n",
    "        for i in range(self.second_stage_depth):\n",
    "            \n",
    "            self.second_stage_encoder_layers.append(snt.Linear(output_size=self.latent_dim))\n",
    "            self.second_stage_encoder_layers.append(tf.nn.relu)\n",
    "            \n",
    "        self.second_stage_loc_head = snt.Linear(output_size=self.latent_dim)\n",
    "        self.second_stage_log_scale_head = snt.Linear(output_size=self.latent_dim)\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        activations = inputs\n",
    "        \n",
    "        for layer in self.second_stage_encoder_layers:\n",
    "            activations = layer(activations)\n",
    "            \n",
    "        # Add residual connection\n",
    "        activations = tf.concat([inputs, activations], axis=-1)\n",
    "        \n",
    "        # Get second stage latent statistics\n",
    "        loc = self.second_stage_loc_head(activations)\n",
    "        log_scale = self.second_stage_log_scale_head(activations)\n",
    "        scale = 1e-6 + tf.nn.softplus(log_scale)\n",
    "        \n",
    "        # Create second stage distribution\n",
    "        self.second_stage_posterior = tfd.Normal(loc=loc,\n",
    "                                                 scale=scale)\n",
    "        \n",
    "        return self.second_stage_posterior.sample()\n",
    "        \n",
    "    \n",
    "    @snt.reuse_variables\n",
    "    def decode_second_stage(self, latents):\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        self.second_stage_decoder_layers = []\n",
    "        \n",
    "        for i in range(self.second_stage_depth):\n",
    "            \n",
    "            self.second_stage_decoder_layers.append(snt.Linear(output_size=self.latent_dim))\n",
    "            self.second_stage_decoder_layers.append(tf.nn.relu)\n",
    "            \n",
    "        self.second_stage_residual_head = snt.Linear(output_size=self.latent_dim)\n",
    "            \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        self.second_stage_prior = tfd.Normal(loc=tf.zeros_like(latents),\n",
    "                                             scale=tf.ones_like(latents))\n",
    "        \n",
    "        activations = latents\n",
    "        \n",
    "        for layer in self.second_stage_decoder_layers:\n",
    "            activations = layer(activations)\n",
    "            \n",
    "        # Add residual connection\n",
    "        activations = tf.concat([latents, activations], axis=-1)\n",
    "        \n",
    "        likelihood_loc = self.second_stage_residual_head(activations)\n",
    "        \n",
    "        return likelihood_loc\n",
    "        \n",
    "    \n",
    "    # =====================================================================\n",
    "    # Build\n",
    "    # =====================================================================\n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        \"\"\"\n",
    "        Build standard VAE:\n",
    "        1. Encode input -> latent mu, sigma\n",
    "        2. Sample z ~ N(z | mu, sigma)\n",
    "        \"\"\"\n",
    "        \n",
    "        reshaper = snt.BatchReshape((28, 28, 1))\n",
    "        inputs = reshaper(inputs)\n",
    "        \n",
    "        # Code the latents on the first stage\n",
    "        latents = self.encode_first_stage(inputs)\n",
    "        \n",
    "        # If the first stage is trained, train the second stage\n",
    "        if self.use_second_stage or self.first_run:\n",
    "            \n",
    "            second_stage_latents = self.encode_second_stage(latents)\n",
    "            latents_ = self.decode_second_stage(second_stage_latents)\n",
    "            \n",
    "            self.latent_log_gamma = tf.get_variable(\"log_gamma_z\", [], tf.float32, tf.zeros_initializer())\n",
    "            self.latent_gamma = tf.exp(self.latent_log_gamma)\n",
    "\n",
    "            # Create likelihood distribution\n",
    "            self.likelihood_second_stage = tfd.Normal(loc=latents,\n",
    "                                                      scale=self.latent_gamma)\n",
    "            \n",
    "            self._log_prob_second_stage = self.likelihood_second_stage.log_prob(latents)\n",
    "            \n",
    "            latents = latents_\n",
    "            \n",
    "            if self.first_run:\n",
    "                self.first_run = False\n",
    "        \n",
    "        # Reconsturct image from the latents\n",
    "        reconstruction = self.decode_first_stage(latents)\n",
    "        \n",
    "        self.log_gamma = tf.get_variable(\"log_gamma_x\", [], tf.float32, tf.zeros_initializer())\n",
    "        self.likelihood = tfd.Normal(loc=reconstruction,\n",
    "                                     scale=tf.exp(self.log_gamma))\n",
    "        self._log_prob = self.likelihood.log_prob(inputs)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 16:51:52.350741 140449970063104 deprecation_wrapper.py:119] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py:177: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n",
      "W0705 16:51:53.129023 140449970063104 deprecation_wrapper.py:119] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py:278: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0705 16:51:53.132635 140449970063104 deprecation_wrapper.py:119] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/base.py:579: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0705 16:51:53.136412 140449970063104 deprecation_wrapper.py:119] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/conv.py:134: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "mvae = MnistTwoStageVAE()\n",
    "\n",
    "test_ones = tf.ones((1, 28, 28))\n",
    "\n",
    "mvae(test_ones)\n",
    "mvae(test_ones)\n",
    "\n",
    "mvae.kl_first_stage\n",
    "mvae.log_prob_first_stage\n",
    "\n",
    "mvae.use_second_stage = True\n",
    "\n",
    "mvae(test_ones)\n",
    "\n",
    "fsv = mvae.get_first_stage_variables()\n",
    "ssv = mvae.get_second_stage_variables()\n",
    "\n",
    "var_names = set(map(lambda x: x.name, fsv)) | set(map(lambda x: x.name, ssv))\n",
    "all_var_names = set(map(lambda x: x.name, mvae.get_all_variables()))\n",
    "\n",
    "# Check if we have all variables captured\n",
    "print(var_names - all_var_names)\n",
    "print(all_var_names - var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(config, model_dir, is_training, train_first_stage=True):\n",
    "\n",
    "    num_batches = config[\"num_training_examples\"] // config[\"batch_size\"] + 1\n",
    "  \n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    ((train_data, _),\n",
    "     (eval_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create model\n",
    "    # ==========================================================================\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    \n",
    "    with g.as_default():\n",
    "    \n",
    "        vae = MnistTwoStageVAE(latent_dim=64)\n",
    "        vae(tf.zeros((1, 28, 28)))\n",
    "        \n",
    "        del vae\n",
    "        \n",
    "    vae = MnistTwoStageVAE(latent_dim=64)\n",
    "    vae(tf.zeros((1, 28, 28)))\n",
    "    \n",
    "    vae.use_second_stage = not train_first_stage\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Checkpoints\n",
    "    # ==========================================================================\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    trainable_vars = vae.get_all_variables() + (global_step,)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "        variables=trainable_vars,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Tensorboard Summary writer\n",
    "    # ==========================================================================\n",
    "\n",
    "    logdir = os.path.join(model_dir, \"log\")\n",
    "    writer = tfs.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    tfs.graph(g)\n",
    "    tfs.flush(writer)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Train the model\n",
    "    # ==========================================================================\n",
    "\n",
    "    if is_training:\n",
    "        \n",
    "        if train_first_stage:\n",
    "            beta = config[\"beta1\"]\n",
    "\n",
    "            for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "                dataset = mnist_input_fn(data=train_data,\n",
    "                                        batch_size=config[\"batch_size\"])\n",
    "\n",
    "                with tqdm(total=num_batches) as pbar:\n",
    "                    for batch in dataset:\n",
    "                        # Increment global step\n",
    "                        global_step.assign_add(1)\n",
    "\n",
    "                        # Record gradients of the forward pass\n",
    "                        with tf.GradientTape() as tape, tfs.record_summaries_every_n_global_steps(config[\"log_freq\"]):\n",
    "\n",
    "                            output = vae(batch)\n",
    "\n",
    "                            kl = vae.kl_first_stage\n",
    "                            total_kl = tf.reduce_sum(kl)\n",
    "\n",
    "                            log_prob = vae.log_prob_first_stage\n",
    "\n",
    "                            warmup_coef = tf.minimum(1., global_step.numpy() / (config[\"warmup\"] * num_batches))\n",
    "\n",
    "                            # negative ELBO\n",
    "                            loss = total_kl - beta * warmup_coef * log_prob \n",
    "\n",
    "                            output = tf.cast(output, tf.float32)\n",
    "\n",
    "                            # Add tensorboard summaries\n",
    "                            tfs.scalar(\"Loss\", loss)\n",
    "                            tfs.scalar(\"Total_KL\", kl)\n",
    "                            tfs.scalar(\"Max_KL\", tf.reduce_max(kl))\n",
    "                            tfs.scalar(\"Log-Probability\", log_prob)\n",
    "                            tfs.scalar(\"Warmup_Coef\", warmup_coef)\n",
    "                            tfs.scalar(\"Gamma-x\", tf.exp(vae.log_gamma))\n",
    "                            tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                        # Backprop\n",
    "                        grads = tape.gradient(loss, vae.get_first_stage_variables())\n",
    "                        optimizer.apply_gradients(zip(grads, vae.get_first_stage_variables()))\n",
    "\n",
    "                        # Update the progress bar\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_description(\"Epoch {}, ELBO: {:.2f}\".format(epoch, loss))\n",
    "\n",
    "                checkpoint.save(ckpt_prefix)\n",
    "\n",
    "            tfs.flush(writer)\n",
    "            print(\"First Stage Training Complete!\")\n",
    "        \n",
    "        vae.use_second_stage = True\n",
    "        \n",
    "        beta = config[\"beta2\"]\n",
    "        \n",
    "        for epoch in range(1, config[\"num_epochs_stage_2\"] + 1):\n",
    "\n",
    "            dataset = mnist_input_fn(data=train_data,\n",
    "                                     batch_size=config[\"batch_size\"])\n",
    "\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for batch in dataset:\n",
    "                    # Increment global step\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                    # Record gradients of the forward pass\n",
    "                    with tf.GradientTape() as tape, tfs.record_summaries_every_n_global_steps(config[\"log_freq\"]):\n",
    "\n",
    "                        output = vae(batch)\n",
    "\n",
    "                        kl = vae.kl_second_stage\n",
    "                        total_kl = tf.reduce_sum(kl)\n",
    "                        \n",
    "                        log_prob = vae.log_prob_second_stage\n",
    "\n",
    "                        warmup_coef = tf.minimum(1., global_step.numpy() / (config[\"warmup\"] * num_batches))\n",
    "\n",
    "                        # negative ELBO\n",
    "                        loss = total_kl - beta * warmup_coef * log_prob \n",
    "\n",
    "                        output = tf.cast(output, tf.float32)\n",
    "\n",
    "                        # Add tensorboard summaries\n",
    "                        tfs.scalar(\"Loss\", loss)\n",
    "                        tfs.scalar(\"Total_KL\", kl)\n",
    "                        tfs.scalar(\"Max_KL\", tf.reduce_max(kl))\n",
    "                        tfs.scalar(\"Log-Probability\", log_prob)\n",
    "                        tfs.scalar(\"Warmup_Coef\", warmup_coef)\n",
    "                        tfs.scalar(\"Gamma-z\", tf.exp(vae.latent_gamma))\n",
    "                        tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                    # Backprop\n",
    "                    grads = tape.gradient(loss, vae.get_second_stage_variables())\n",
    "                    optimizer.apply_gradients(zip(grads, vae.get_second_stage_variables()))\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(\"Epoch {}, ELBO: {:.2f}\".format(epoch, loss))\n",
    "\n",
    "            checkpoint.save(ckpt_prefix)\n",
    "            \n",
    "        tfs.flush(writer)\n",
    "        print(\"Second Stage Training Complete!\")\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 250,\n",
      "    \"beta1\": 0.1,\n",
      "    \"beta2\": 0.1,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"log_freq\": 100,\n",
      "    \"num_epochs\": 5,\n",
      "    \"num_epochs_stage_2\": 5,\n",
      "    \"num_training_examples\": 60000,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"warmup\": 10.0\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 16:52:17.449425 140449970063104 deprecation.py:506] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/conv.py:298: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0705 16:52:17.456937 140449970063104 deprecation.py:506] From /homes/gf332/Documents/projects/VAEs/vae_venv/lib/python3.7/site-packages/sonnet/python/modules/conv.py:303: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found at /tmp/2-stage-vae/checkpoints/_ckpt-5, restoring...\n",
      "Model restored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035579b46cb946c6b10cb64b62147ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1fe5c163c0478b954529a9a4adeea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de65aa1549fa49b3bfe4cca6edb31494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37183d4358fb4271862804e84dc1d35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29b627dda3643fabafb00374277b9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Stage Training Complete!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aae11f0ffa64e08818f34fbcd61098c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efc1617c31a4e08bca791c3c13a11b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8746825b7d11486e90e641fb41687a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a208cfd1f945c9bb0c7ccafc4aa341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2452d4495941189806a7fbee0375a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second Stage Training Complete!\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"/tmp/2-stage-vae/\"\n",
    "\n",
    "config = {\n",
    "        \"num_training_examples\": 60000,\n",
    "        \"batch_size\": 250,\n",
    "        \"num_epochs\": 5,\n",
    "        \"num_epochs_stage_2\": 5,\n",
    "        \n",
    "        \"beta1\": 0.1,\n",
    "        \"beta2\": 0.1,\n",
    "        \"warmup\": 10.,\n",
    "        \n",
    "        \"learning_rate\": 1e-3,\n",
    "        \n",
    "        \"optimizer\": \"adam\",\n",
    "        \n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "        \"log_freq\": 100,\n",
    "    }\n",
    "\n",
    "vae = run(config, model_dir=MODEL_DIR, is_training=True, train_first_stage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1153684, shape=(1, 28, 28, 3), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt.BatchNorm()(tf.ones((1, 28, 28, 3)), is_training=False, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
