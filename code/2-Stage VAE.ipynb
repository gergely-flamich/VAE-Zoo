{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Stage VAE\n",
    "\n",
    "Based on [Diagnosing and Enhancing VAE Models](https://arxiv.org/abs/1903.05789) by Dai and Wipf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, glob\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfs = tf.contrib.summary\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "import sonnet as snt\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_input_fn(data, batch_size=256, shuffle_samples=5000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(shuffle_samples)\n",
    "    dataset = dataset.map(mnist_parse_fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mnist_parse_fn(data):\n",
    "    return tf.cast(data, tf.float32) / 255.\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=True),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistVAE(snt.AbstractModule):\n",
    "\n",
    "    def __init__(self, latent_dim=2, name=\"mnist_vae\"):\n",
    "\n",
    "        super(MnistVAE, self).__init__(name=name)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.is_training = True\n",
    "\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        \"\"\"\n",
    "        Calculates the KL divergence between the current variational posterior and the prior:\n",
    "\n",
    "        KL[ q(z | theta) || p(z) ]\n",
    "\n",
    "        \"\"\"\n",
    "        self._ensure_is_connected()\n",
    "        \n",
    "        return tfd.kl_divergence(self.latent_posterior, self.latent_prior)\n",
    "\n",
    "    @property\n",
    "    def input_log_prob(self):\n",
    "        \"\"\"\n",
    "        Returns the log-likelihood of the current input for the output Bernoulli\n",
    "        \"\"\"\n",
    "        return tf.reduce_sum(self.log_prob)\n",
    "\n",
    "\n",
    "    @snt.reuse_variables\n",
    "    def encode(self, inputs):\n",
    "        \"\"\"\n",
    "        Builds the encoder part of the VAE, i.e. q(x | theta).\n",
    "        This maps from the input to the latent representation.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        self.encoder_layers = [\n",
    "            snt.Conv2D(output_channels=64,\n",
    "                       kernel_shape=(5, 5),\n",
    "                       stride=2),\n",
    "            tf.nn.leaky_relu,\n",
    "            snt.Conv2D(output_channels=128,\n",
    "                       kernel_shape=(5, 5),\n",
    "                       stride=2,\n",
    "                       use_bias=False),\n",
    "            snt.BatchNorm(),\n",
    "            tf.nn.leaky_relu,\n",
    "            snt.BatchFlatten(),\n",
    "            snt.Linear(output_size=1024,\n",
    "                       use_bias=False),\n",
    "            snt.BatchNorm(),\n",
    "            tf.nn.leaky_relu\n",
    "        ]\n",
    "        \n",
    "        self.encoder_loc_head = snt.Linear(output_size=self.latent_dim)\n",
    "        self.encoder_log_scale_head = snt.Linear(output_size=self.latent_dim)\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        activations = inputs\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            if isinstance(layer, snt.BatchNorm):\n",
    "                activations = layer(activations, is_training=self.is_training)\n",
    "            else:\n",
    "                activations = layer(activations)\n",
    "            \n",
    "        # Get latent posterior statistics\n",
    "        loc = self.encoder_loc_head(activations)\n",
    "        scale = 1e-6 + tf.nn.softplus(self.encoder_log_scale_head(activations))\n",
    "        \n",
    "        # Create latent posterior\n",
    "        self.latent_posterior = tfd.Normal(loc=loc, scale=scale)\n",
    "        \n",
    "        return self.latent_posterior.sample()\n",
    "\n",
    "\n",
    "    @snt.reuse_variables\n",
    "    def decode(self, latent_code):\n",
    "        \"\"\"\n",
    "        Builds the decoder part of the VAE\n",
    "        \"\"\"\n",
    "        # ----------------------------------------------------------------\n",
    "        # Define Layers\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        self.decoder_levels = [self.encoder_loc_head.transpose()]\n",
    "        \n",
    "        for layer in self.encoder_layers[::-1]:\n",
    "            \n",
    "            if isinstance(layer, snt.Transposable):\n",
    "                self.decoder_levels.append(layer.transpose())\n",
    "                \n",
    "            else:\n",
    "                if isinstance(layer, snt.BatchFlatten):\n",
    "                    self.decoder_levels.append(BatchReshape((28, 28, 1)))\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Apply Layers\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        # Create prior\n",
    "        self.latent_prior = tfd.Normal(loc=tf.zeros_like(latent_code),\n",
    "                                       scale=tf.ones_like(latent_code))\n",
    "        \n",
    "        activations = latent_code\n",
    "        \n",
    "        for layer in self.decoder_levels:\n",
    "            activations = layer(activations)\n",
    "        \n",
    "        return tf.nn.sigmoid(activations)\n",
    "\n",
    "\n",
    "    def _build(self, inputs):\n",
    "        \"\"\"\n",
    "        Build standard VAE:\n",
    "        1. Encode input -> latent mu, sigma\n",
    "        2. Sample z ~ N(z | mu, sigma)\n",
    "        \"\"\"\n",
    "        \n",
    "        reshaper = snt.BatchReshape((28, 28, 1))\n",
    "        inputs = reshaper(inputs)\n",
    "        \n",
    "        latents = self.encode(inputs)\n",
    "        reconstruction = self.decode(latents)\n",
    "        \n",
    "        self.log_gamma = tf.get_variable(\"log_gamma_x\", [], tf.float32, tf.zeros_initializer())\n",
    "        self.likelihood = tfd.Normal(loc=reconstruction,\n",
    "                                     scale=tf.exp(self.log_gamma))\n",
    "        self.log_prob = self.likelihood.log_prob(inputs)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(config, model_dir, is_training):\n",
    "\n",
    "    num_batches = config[\"num_training_examples\"] // config[\"batch_size\"] + 1\n",
    "  \n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    ((train_data, _),\n",
    "     (eval_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create model\n",
    "    # ==========================================================================\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    \n",
    "    with g.as_default():\n",
    "    \n",
    "        vae = MnistVAE()\n",
    "        vae(tf.zeros((1, 28, 28)))\n",
    "        \n",
    "        del vae\n",
    "        \n",
    "    vae = MnistVAE()\n",
    "    vae(tf.zeros((1, 28, 28)))\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Checkpoints\n",
    "    # ==========================================================================\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    trainable_vars = vae.get_all_variables() + (global_step,)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "        variables=trainable_vars,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Tensorboard Summary writer\n",
    "    # ==========================================================================\n",
    "\n",
    "    logdir = os.path.join(model_dir, \"log\")\n",
    "    writer = tfs.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    tfs.graph(g)\n",
    "    tfs.flush(writer)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Train the model\n",
    "    # ==========================================================================\n",
    "\n",
    "    beta = config[\"beta\"]\n",
    "\n",
    "    if is_training:\n",
    "        for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "            dataset = mnist_input_fn(data=train_data,\n",
    "                                    batch_size=config[\"batch_size\"])\n",
    "\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for batch in dataset:\n",
    "                    # Increment global step\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                    # Record gradients of the forward pass\n",
    "                    with tf.GradientTape() as tape, tfs.record_summaries_every_n_global_steps(config[\"log_freq\"]):\n",
    "\n",
    "                        output = vae(batch)\n",
    "\n",
    "                        kl = vae.kl_divergence \n",
    "                        total_kl = tf.reduce_sum(kl)\n",
    "                        \n",
    "                        log_prob = vae.input_log_prob\n",
    "\n",
    "                        warmup_coef = tf.minimum(1., global_step.numpy() / (config[\"warmup\"] * num_batches))\n",
    "\n",
    "                        # negative ELBO\n",
    "                        loss = total_kl - beta * warmup_coef * log_prob \n",
    "\n",
    "                        output = tf.cast(tf.expand_dims(output, axis=-1), tf.float32)\n",
    "\n",
    "                        # Add tensorboard summaries\n",
    "                        tfs.scalar(\"Loss\", loss)\n",
    "                        tfs.scalar(\"Total_KL\", kl)\n",
    "                        tfs.scalar(\"Max_KL\", tf.reduce_max(kl))\n",
    "                        tfs.scalar(\"Log-Probability\", log_prob)\n",
    "                        tfs.scalar(\"Warmup_Coef\", warmup_coef)\n",
    "                        tfs.scalar(\"Gamma-x\", tf.exp(vae.log_gamma))\n",
    "                        tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                    # Backprop\n",
    "                    grads = tape.gradient(loss, vae.get_all_variables())\n",
    "                    optimizer.apply_gradients(zip(grads, vae.get_all_variables()))\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(\"Epoch {}, ELBO: {:.2f}\".format(epoch, loss))\n",
    "\n",
    "            checkpoint.save(ckpt_prefix)\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 250,\n",
      "    \"beta\": 1.0,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"log_freq\": 100,\n",
      "    \"num_epochs\": 40,\n",
      "    \"num_training_examples\": 60000,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"warmup\": 20.0\n",
      "}\n",
      "No checkpoint found!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9281e16ec42c4eaa94b173e66ee3ac24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-280a2842e9f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     }\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e3d960a386e5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config, model_dir, is_training)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1130\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Miscellaneous/VAE/vae_venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5307\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5308\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5309\u001b[0;31m         \"transpose_b\", transpose_b)\n\u001b[0m\u001b[1;32m   5310\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5311\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"/tmp/2-stage-vae/\"\n",
    "\n",
    "config = {\n",
    "        \"num_training_examples\": 60000,\n",
    "        \"batch_size\": 250,\n",
    "        \"num_epochs\": 40,\n",
    "        \n",
    "        \"beta\": 1.,\n",
    "        \"warmup\": 20.,\n",
    "        \n",
    "        \"learning_rate\": 1e-3,\n",
    "        \n",
    "        \"optimizer\": \"adam\",\n",
    "        \n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "        \"log_freq\": 100,\n",
    "    }\n",
    "\n",
    "vae = run(config, model_dir=MODEL_DIR, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
